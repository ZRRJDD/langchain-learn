{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   driver_id           event_timestamp  conv_rate  acc_rate  avg_daily_trips\n",
      "0       1001 2021-04-12 10:59:42+00:00   0.769165  0.467336              690\n",
      "1       1002 2021-04-12 08:12:10+00:00   0.565716  0.084002              212\n",
      "2       1003 2021-04-12 16:40:26+00:00   0.283134  0.544876              997\n",
      "3       1004 2021-04-12 15:01:12+00:00   0.532676  0.634407              830\n"
     ]
    }
   ],
   "source": [
    "from feast import FeatureStore\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "entity_df = pd.DataFrame.from_dict({\n",
    "    \"driver_id\": [1001, 1002, 1003, 1004],\n",
    "    \"event_timestamp\": [\n",
    "        datetime(2021, 4, 12, 10, 59, 42),\n",
    "        datetime(2021, 4, 12, 8,  12, 10),\n",
    "        datetime(2021, 4, 12, 16, 40, 26),\n",
    "        datetime(2021, 4, 12, 15, 1 , 12)\n",
    "    ]\n",
    "})\n",
    "\n",
    "feast_repo_path = \"/Users/zrrjdd/software/github/langchain-learn/myfeature_repo/feature_repo/\"\n",
    "store = FeatureStore(repo_path=feast_repo_path)\n",
    "\n",
    "training_df = store.get_historical_features(\n",
    "    entity_df=entity_df,\n",
    "    features = [\n",
    "        'driver_hourly_stats:conv_rate',\n",
    "        'driver_hourly_stats:acc_rate',\n",
    "        'driver_hourly_stats:avg_daily_trips'\n",
    "    ],\n",
    ").to_df()\n",
    "\n",
    "print(training_df.head())\n",
    "\n",
    "# Train model\n",
    "# model = ml.fit(training_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feast import FeatureStore\n",
    "from langchain.prompts import PromptTemplate, StringPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "feast_repo_path = \"/Users/zrrjdd/software/github/langchain-learn/myfeature_repo/feature_repo/\"\n",
    "store = FeatureStore(repo_path=feast_repo_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Given the driver's up to date stats, write them note relaying those stats to them.\n",
    "If they have a conversation rate above .5, give them a compliment. Otherwise, make a silly joke about chickens at the end to make them feel better\n",
    "\n",
    "Here are the drivers stats:\n",
    "Conversation rate: {conv_rate}\n",
    "Acceptance rate: {acc_rate}\n",
    "Average Daily Trips: {avg_daily_trips}\n",
    "\n",
    "Your response:\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeastPromptTemplate(StringPromptTemplate):\n",
    "    \n",
    "    def format(self, **kwargs) -> str:\n",
    "        driver_id = kwargs.pop(\"driver_id\")\n",
    "        feature_vector = store.get_online_features(\n",
    "            features=[\n",
    "                'driver_hourly_stats:conv_rate',\n",
    "                'driver_hourly_stats:acc_rate',\n",
    "                'driver_hourly_stats:avg_daily_trips'\n",
    "            ],\n",
    "            entity_rows=[{\"driver_id\": driver_id}]\n",
    "        ).to_dict()\n",
    "        print(feature_vector)\n",
    "        kwargs[\"conv_rate\"] = feature_vector[\"conv_rate\"][0]\n",
    "        kwargs[\"acc_rate\"] = feature_vector[\"acc_rate\"][0]\n",
    "        kwargs[\"avg_daily_trips\"] = feature_vector[\"avg_daily_trips\"][0]\n",
    "        return prompt.format(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = FeastPromptTemplate(input_variables=[\"driver_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'driver_id': [1001], 'acc_rate': [None], 'conv_rate': [None], 'avg_daily_trips': [None]}\n",
      "Given the driver's up to date stats, write them note relaying those stats to them.\n",
      "If they have a conversation rate above .5, give them a compliment. Otherwise, make a silly joke about chickens at the end to make them feel better\n",
      "\n",
      "Here are the drivers stats:\n",
      "Conversation rate: None\n",
      "Acceptance rate: None\n",
      "Average Daily Trips: None\n",
      "\n",
      "Your response:\n"
     ]
    }
   ],
   "source": [
    "print(prompt_template.format(driver_id=1001))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 创建自定义提示模版"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "\n",
    "def get_source_code(function_name):\n",
    "    # Get the source code of the function\n",
    "    return inspect.getsource(function_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import StringPromptTemplate\n",
    "from pydantic import BaseModel, validator\n",
    "\n",
    "\n",
    "class FunctionExplainerPromptTemplate(StringPromptTemplate, BaseModel):\n",
    "    \"\"\" A custom prompt template that takes in the function name as input, and formats the prompt template to provide the source code of the function. \"\"\"\n",
    "\n",
    "    @validator(\"input_variables\")\n",
    "    def validate_input_variables(cls, v):\n",
    "        \"\"\" Validate that the input variables are correct. \"\"\"\n",
    "        if len(v) != 1 or \"function_name\" not in v:\n",
    "            raise ValueError(\"function_name must be the only input_variable.\")\n",
    "        return v\n",
    "\n",
    "    def format(self, **kwargs) -> str:\n",
    "        # Get the source code of the function\n",
    "        source_code = get_source_code(kwargs[\"function_name\"])\n",
    "\n",
    "        # Generate the prompt to be sent to the language model\n",
    "        prompt = f\"\"\"\n",
    "        Given the function name and source code, generate an English language explanation of the function.\n",
    "        Function Name: {kwargs[\"function_name\"].__name__}\n",
    "        Source Code:\n",
    "        {source_code}\n",
    "        Explanation:\n",
    "        \"\"\"\n",
    "        return prompt\n",
    "    \n",
    "    def _prompt_type(self):\n",
    "        return \"function-explainer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Given the function name and source code, generate an English language explanation of the function.\n",
      "        Function Name: get_source_code\n",
      "        Source Code:\n",
      "        def get_source_code(function_name):\n",
      "    # Get the source code of the function\n",
      "    return inspect.getsource(function_name)\n",
      "\n",
      "        Explanation:\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "fn_explainer = FunctionExplainerPromptTemplate(input_variables=[\"function_name\"])\n",
    "\n",
    "# Generate a prompt for the function \"get_source_code\"\n",
    "prompt = fn_explainer.format(function_name=get_source_code)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "This function takes in a function name as an argument and returns the source code of that function. It does this by using the inspect module to get the source code of the given function.\n"
     ]
    }
   ],
   "source": [
    "r = llm(prompt=prompt)\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
