{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 环境配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# 加载.env\n",
    "load_dotenv('.env')\n",
    "# 配置openai api key\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI()\n",
    "chat_model = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'，我是自动机器人，很高兴为你服务！'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm('你好')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='你好！有什么我可以为您效劳的吗？', additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_model(messages=[HumanMessage(content=\"你好\")])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(model_name=\"text-ada-001\",n=2,best_of=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n一杯茶,一杯丁饭,一起去看电视吧!\\n\\n影'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(\"给我讲个笑话\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate: More broadly, you can call it with a list of inputs, getting back a more complete response than just the text. This complete response includes things like multiple top responses, as well as LLM provider specific information\n",
    "\n",
    "生成：更广泛地说，您可以使用输入列表调用它，获得比仅文本更完整的响应。这个完整的响应包括多个顶级响应以及LLM提供者特定信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMResult(generations=[[Generation(text='\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side.', generation_info={'finish_reason': 'stop', 'logprobs': None}), Generation(text='\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side!', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text=\"\\n\\nThere's no one way to be\\n\\nThere's no one way to be\\n\\nThere's no one way to be\\n\\nThere's just me,\\n\\nAnd you, and you, and I,\\n\\nAnd no one way to be\\n\\nThere's no one way to be\\n\\nThere's just me,\\n\\nAnd you, and you, and I,\\n\\nAnd no one way to be\\n\\nThere's just me,\\n\\nAnd you, and you, and I,\\n\\nAnd I think of you\\n\\nWhen I'm feeling down\\n\\n'Cause I'm sure as long\\n\\nAs I'm alive I'll\\n\\nBeicitous of you\\n\\nI can't tell you\\n\\nWhat's wrong\\n\\nI know I should go\\n\\nBut I can't help but think\\n\\nOf you, and I care for you\\n\\nWhen I'm feeling down\\n\\nI know I should go\\n\\nBut I can't help but think\\n\\nOf you, and I care for you\\n\\nWhen I'm feeling down\\n\\nI know I should go\\n\\nBut I can't help but think\\n\\nOf you, and I care for you\", generation_info={'finish_reason': None, 'logprobs': None}), Generation(text=\"\\n\\nThere's no need for words\\n\\nWhen we're with each other\\n\\nWe'll find a way to be so content\\n\\nAnd find a way to be each other's light\\n\\nWe'll find a way to be so content\\n\\nWhen we're together\\n\\nWe'll find a way to be so content\\n\\nAnd find a way to be each other's light\", generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side.', generation_info={'finish_reason': 'stop', 'logprobs': None}), Generation(text='\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side.', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text=\"\\n\\nHow can I love\\nI have never seen\\nIn my life\\nI feel like I'm in love\\nWith you\\nI think love is strong\\nAnd it can be Trafford\\nI think love is strong\\nAnd it can be Trafford\\n\\nWe all have ways of expressing\\nOur feelings\\nWe all have ways of how we feel\\nWe all have ways of how we think\\n\\nIt's all about making it work\\nYou're the one\\nYou're the one\\nYou're the one\\n\\nI think love is strong\\nAnd it can be Trafford\\nI think love is strong\\nAnd it can be Trafford\\nI think love is strong\\nAnd it can be Trafford\\nWe all have ways of expressing\\nOur feelings\\nWe all have ways of how we feel\\nWe all have ways of how we think\", generation_info={'finish_reason': None, 'logprobs': None}), Generation(text='\\n\\nI can\\'t pick a love poem\\n\\nI choose \"The Defeated Wave\"\\n\\nThe love story of a man and a woman,\\n\\nWho finally lose each other,\\n\\nIn a moment ofenbergation\\n\\nBut eventually find each other again\\n\\nIn a place where love is once again\\n\\nA powerful force, and always together\\n\\nA2 The Defeated Wave\\n\\nA love story told by two people,\\n\\nWho finally lose each other,\\n\\nIn a moment of despair and Vide\\n\\nBut eventually find each other again\\n\\nIn a place where love is once again\\n\\nA powerful force, and always together\\n\\nA2 The Defeated Wave', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side.', generation_info={'finish_reason': 'stop', 'logprobs': None}), Generation(text='\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side!', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\nWhen I was younger\\nI thought that love\\nI was something like a fairytale\\nI would find my prince and they would be my people\\nI was naïve\\nI thought that love\\nwas something like a fairytale\\n\\nNow I know that love\\nIs something that should be sought\\nAnd that love is something that should be shared\\n\\nWith someone that I care for\\nAnd that love\\nIs something that should be cherished\\nAnd that love\\nIs something that should be valued\\n\\nNow I know that love\\nIs something that should be earned', generation_info={'finish_reason': 'stop', 'logprobs': None}), Generation(text=\"\\n\\nIn the eyes of the moon\\n\\nI have seen a face\\n\\nThat I will never forget.\\n\\nIts beauty is so divine\\n\\nI can't imagine life without it.\\n\\n Its 1,000 calories per hour\\n\\nThe thoughts that she brings to my mind\\n\\nI can't help but smile\\n\\nWhen I think about all the things we share.\\n\\nThere's no way that I can help\\n\\nWith all the things that I've been through\\n\\nBut I know that I will always love it.\", generation_info={'finish_reason': None, 'logprobs': None})], [Generation(text='\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side!', generation_info={'finish_reason': 'stop', 'logprobs': None}), Generation(text='\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side.', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text=\"\\n\\nA rose by the side of the road\\n\\nIs all I need to find my way\\n\\nTo the place I've been searching for\\n\\nAnd my heart is singing with joy\\n\\nWhen I look at this rose\\n\\nIt reminds me of the love I've found\\n\\nAnd I know that wherever I go\\n\\nI'll always find my rose by the side of the road.\", generation_info={'finish_reason': 'stop', 'logprobs': None}), Generation(text=\"\\n\\nA rose by the side of the road\\n\\nIs all I need to find my way\\n\\nTo the place I've been searching for\\n\\nAnd my heart is singing with joy\\n\\nWhen I look at this rose\\n\\nIt reminds me of the love I've found\\n\\nAnd I know that wherever I go\\n\\nI'll always find my rose by the side of the road.\", generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side!', generation_info={'finish_reason': 'stop', 'logprobs': None}), Generation(text='\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side!', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text=\"\\n\\nHow can love never truly exist\\n\\nWhen everything we love is lost\\n\\nAnd we all justiws that we've ever had\\n\\nA love so strong and so true\\n\\nCan we never really mean it\\n\\nWhen we say our love was created\\n\\nAnd we no longer feel the same\\n\\nThat we once did about each other\\n\\nWe love each other so much more\\n\\nAnd I think that's enough\\n\\nFor now, but I'm not sure\\n\\nWhat comes after love\\n\\nIs what we'll never know\\n\\nBut we always will love\\n\\nEach other through everything\\n\\nAnd anything is possible\\n\\nFor us to come together\\n\\nAnd share our love once more\\n\\nOn into the future\\n\\nAnd I am so excited for this\\n\\nI think we can be together\\n\\nAnd love is back\\n\\nAnd it's so strong and true\\n\\nIt's always been with us\\n\\nAnd we'll never forget\\n\\nWhat we have for each other\\n\\nWe are so happy\\n\\nWe have the best life ever\\n\\nHe lives inside of me\\n\\nAnd I think we can be this way\\n\\nWhen we share our love\\n\\nIt always means so much\\n\\n\", generation_info={'finish_reason': 'length', 'logprobs': None}), Generation(text=\"\\n\\nA rose by the side of the road\\n\\nIs all I need to find my way\\n\\nTo the place I've been searching for\\n\\nAnd my heart is singing with joy\\n\\nWhen I look at this rose\\n\\nIt reminds me of the love I've found\\n\\nAnd I know that wherever I go\\n\\nI'll always find my rose by the side of the road.\", generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side.', generation_info={'finish_reason': 'stop', 'logprobs': None}), Generation(text='\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side.', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text=\"\\n\\nThe colors of the sky are so bright\\nAnd the sun is so warm\\nI can't wait to go back\\nTo this beautiful place\\nAnd I can't wait to see\\nThe things we've seen before\\nIn our lifetime we'll see\\n\\nWe'll make it through\\nWe'll make it through\\n\\nWe'll make it through\\n\\nWe'll make it through\\n\\nOur lives are only a game\\nBut we'll make it through\\n\\nOur lives are only a game\\nBut we'll make it through\", generation_info={'finish_reason': 'stop', 'logprobs': None}), Generation(text=\"\\n\\nI read about the\\n\\nGreat white north\\n\\nAnd about the guns\\n\\nOf the' Maddie\\n\\nAnd about the\\n\\n Antarctic\\n\\nAnd about my\\n\\nFirst love\\n\\nAnd about to\\n\\n grow old\\n\\nAnd about the\\n\\nBlessed are the\\n\\nWho shear their\\n\\nHair\\n\\nAnd about the\\n\\n Tears\\n\\nAnd about\\n\\nThe\\n\\nSorrow\\n\\nAnd about\\n\\nThe\\n\\nBlessed are the\\n\\nThat her\\n\\nWorked their\\n\\nHair\\n\\nAnd about\\n\\nThe\\n\\nSorrow\\n\\nAnd about\\n\\nThe\\n\\nBlessed are the\\n\\nThat her\\n\\nWorked their\\n\\nHair\\n\\nAnd about\\n\\nThe\\n\\nSorrow\\n\\nAnd about\\n\\nThe\\n\\nBlessed are the\\n\\nThat she\\n\\nWorked their\\n\\nHair\", generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side.', generation_info={'finish_reason': 'stop', 'logprobs': None}), Generation(text='\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side!', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\nHow could I\\n\\nHow could I\\n\\nWhy did the\\n\\nThe sky have to be\\n\\nThe sky had to be\\n\\nThe sky had to be\\n\\nThe sky had to be\\n\\nWhy did the\\n\\nThe sky had to be\\n\\nThe sky had to be\\n\\nThe sky had to be\\n\\nThe sky had to be\\n\\nWhy did the\\n\\nThe sky had to be\\n\\nThe sky had to be\\n\\nThe sky had to be\\n\\nThe sky had to be\\n\\nWhy did the\\n\\nThe sky had to be\\n\\nThe sky had to be\\n\\nThe sky had to be\\n\\nThe sky had to be\\n\\nWhy did the\\n\\nThe sky had to be\\n\\nThe sky had to be\\n\\nThe sky had to be\\n\\nThe sky had to be\\n\\nWhy did the\\n\\nThe sky had to be\\n\\nThe sky had to be\\n\\nThe sky had to be\\n\\nThe sky had to be\\n\\nWhy did the\\n\\nThe sky had to be\\n\\nThe sky had to be\\n\\nThe sky had to be\\n\\nThe sky had to be\\n\\nWhy did the\\n\\nThe sky had to be\\n\\nThe', generation_info={'finish_reason': 'length', 'logprobs': None}), Generation(text=\"\\n\\nA rose by the side of the road\\n\\nIs all I need to find my way\\n\\nTo the place I've been searching for\\n\\nAnd my heart is singing with joy\\n\\nWhen I look at this rose\\n\\nIt tells me that I've beensearching for a long time\\n\\nAnd it reminds me of the love I've found\\n\\nAnd I can't help but be happy\\n\\nSo I take it as a sign\\n\\nAnd I goodnight, rose by the side of the road\", generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side!', generation_info={'finish_reason': 'stop', 'logprobs': None}), Generation(text='\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side.', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text=\"\\n\\nHow can a little thing like me\\n\\nEver be considered perfect?\\n\\nWhat is it like to know\\n\\nThat you're always there in the dark?\\n\\nHow can a little thing like me\\n\\nEver be considered perfect?\\n\\nIs it always about the money?\\n\\nIs it always about the title?\\n\\nIs it always about the market?\\n\\nIs it always about the sales?\\n\\nIs it always about the credit?\\n\\nIs it always about the book?\\n\\nIs it always about the sales?\\n\\nIs it always about the perfect book?\\n\\nIs it always about being the best?\\n\\nIs it always about making money?\\n\\nIs it always about making sales?\\n\\nIs it always about being the perfect book?\\n\\nIs it always about being the perfect book?\", generation_info={'finish_reason': 'stop', 'logprobs': None}), Generation(text='\\n\\nIn the face of danger\\n\\nWe will fight\\n\\nWith love and light\\n\\nWe will make it through\\n\\nTo the end that we are wanted\\n\\nAnd we will come back to you\\n\\nIn the face of danger\\n\\nWe will fight\\n\\nWith love and light\\n\\nWe will make it through\\n\\nTo the end that we are wanted\\n\\nAnd we will come back to you', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side.', generation_info={'finish_reason': 'stop', 'logprobs': None}), Generation(text='\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side!', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text=\"\\n\\nA rose by the side of the road\\n\\nIs all I need to find my way\\n\\nTo the place I've been searching for\\n\\nAnd my heart is singing with joy\\n\\nWhen I look at this rose\\n\\nIt reminds me of the love I've found\\n\\nAnd I know that wherever I go\\n\\nI'll always find my rose by the side of the road.\", generation_info={'finish_reason': 'stop', 'logprobs': None}), Generation(text=\"\\n\\nI can't pick a love poem\\n\\nSomebody help me\\n\\nI'm in search of a love poem\\n\\nI want to know what you think\\n\\nI want to know what you think\\n\\nI can't find my way out\\n\\nI can't find my way out\\n\\nI just want to know what you think\\n\\nI want to know what you think\\n\\nOf the love poem I find\\n\\nOf the love poem I find\", generation_info={'finish_reason': None, 'logprobs': None})], [Generation(text='\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side.', generation_info={'finish_reason': 'stop', 'logprobs': None}), Generation(text='\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side!', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text=\"\\n\\nThere's no one to turn to\\n\\nOnly the market's honest toil\\n\\nAnd the The Israelites\\n\\nμ’am-tli»»»\\n\\nking, god, and prophets\\n\\n convey to me their wisdom\\n\\nAnd the hosts of God \\n\\nA song for the new century\\n\\nThe opportunity is here\\n\\nTo take us all in\\n\\nThe coming of the just will\\n\\nThe delayed hand of time\\n\\nWe are his, just as we are\\n\\nAnd the poem is over\", generation_info={'finish_reason': 'stop', 'logprobs': None}), Generation(text=\"\\n\\nA rose by the side of the road\\n\\nIs all I need to find my way\\n\\nTo the place I've been searching for\\n\\nAnd my heart is singing with joy\\n\\nWhen I look at this rose\\n\\nIt reminds me of the love I've found\\n\\nAnd I know that wherever I go\\n\\nI'll always find my rose by the side of the road.\", generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side!', generation_info={'finish_reason': 'stop', 'logprobs': None}), Generation(text='\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side.', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text=\"\\n\\nIn the eyes of the moon\\n\\nI have seen a face\\n\\nThat I will never forget.\\n\\nIts beauty is so divine\\n\\nI can't imagine life without it.\\n\\nIts light is so powerful\\n\\nI can't imagine my life\\n\\n WITHOUT it.\", generation_info={'finish_reason': 'stop', 'logprobs': None}), Generation(text=\"\\n\\nIn the eyes of the moon\\n\\nI have seen a face\\n\\nThat I will never forget.\\n\\nIts beauty is so divine\\n\\nI can't imagine life without it.\\n\\nIts pain is real\\n\\nI have felt it more than once.\\n\\nBut its face that I have loved\\n\\nWill be my favorite memory\\n\\nOf the moon.\", generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side!', generation_info={'finish_reason': 'stop', 'logprobs': None}), Generation(text='\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side!', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\nWe are all broken\\n\\nEach piece of us\\n\\nBleeds out in the rain\\n\\nBut we try to mend\\n\\nAnd hope for a better tomorrow.', generation_info={'finish_reason': 'stop', 'logprobs': None}), Generation(text='\\n\\nNo one ever comes\\n\\nto the party you just\\n\\ngot started on.\\n\\nNo one ever comes\\n\\nto the party you just\\n\\ngot started on.', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side!', generation_info={'finish_reason': 'stop', 'logprobs': None}), Generation(text='\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side!', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text=\"\\n\\nWhen you're gone,\\n\\nI feel like this is my home.\\n\\nI can't wait to see you again,\\n\\nYou always make me feel so welcome.\\n\\nI love the way you laugh,\\n\\nIt makes me feel so good.\\n\\nWhen you're gone,\\n\\nI feel like this is my home.\\n\\nI can't wait to see you again,\\n\\nYou always make me feel so welcome.\", generation_info={'finish_reason': 'stop', 'logprobs': None}), Generation(text=\"\\n\\nHow can a little love be\\n\\nMakes the world go round\\n\\nIn my opinion\\n\\nLove is the only way\\n\\nTo make a way where you're free\\n\\nTo do what you love\\n\\nTo the moon and back\\n\\nLove is the only way\\n\\nTo make a way where you're free\\n\\nTo do what you love\\n\\nTo the moon and back\\n\\nLove is the only way\\n\\nTo make a way where you're free\\n\\nTo do what you love\\n\\nTo the moon and back\", generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side.', generation_info={'finish_reason': 'stop', 'logprobs': None}), Generation(text='\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side.', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\nHow can a little love\\n\\n Conquer everything?\\n\\nHow can it be that so much love\\n\\nIs never enough?', generation_info={'finish_reason': 'stop', 'logprobs': None}), Generation(text='\\n\\nHow do I love\\n\\nHow do I love\\n\\nA lie, a lie,\\n\\nA lie, a lie,\\n\\nA lie, a lie,\\n\\nHow do I love\\n\\nHow do I love\\n\\nA lie, a lie,\\n\\nA lie, a lie,\\n\\nA lie, a lie,\\n\\nHow do I love\\n\\nHow do I love\\n\\nA lie, a lie,\\n\\nA lie, a lie,\\n\\nA lie, a lie,\\n\\nHow do I love\\n\\nHow do I love\\n\\nA lie, a lie,\\n\\nA lie, a lie,\\n\\nA lie, a lie,\\n\\nHow do I love\\n\\nHow do I love\\n\\nA lie, a lie,\\n\\nA lie, a lie,\\n\\nA lie, a lie,\\n\\nHow do I love', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side.', generation_info={'finish_reason': 'stop', 'logprobs': None}), Generation(text='\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side.', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text=\"\\n\\nThe world is a beautiful place\\nThe colors are so bright and true\\nAnd I feel so free and free\\nWhen I'm away from here\\nThe free world is waiting for me\\nAnd I'm going to enjoy it so much\\nThat I might even move to here\\nI'm so content here\\nAnd I'm so happy\\n\\nThe world is a beautiful place\\nAnd I feel so free and free\\nWhen I'm away from here\\nThe free world is waiting for me\\nAnd I'm going to enjoy it so much\\nThat I might even move to here\\nI'm so content here\", generation_info={'finish_reason': 'stop', 'logprobs': None}), Generation(text=\"\\n\\nA rose by the side of the road\\n\\nIs all I need to find my way\\n\\nTo the place I've been searching for\\n\\nAnd my heart is singing with joy\\n\\nWhen I look at this rose\\n\\nIt reminds me of the love I've found\\n\\nAnd I know that wherever I go\\n\\nI'll always find my rose by the side of the road.\", generation_info={'finish_reason': 'stop', 'logprobs': None})]], llm_output={'token_usage': {'total_tokens': 4202, 'prompt_tokens': 120, 'completion_tokens': 4082}, 'model_name': 'text-ada-001'})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_result = llm.generate([\"Tell me a joke\", \"Tell me a poem\"]*15)\n",
    "llm_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(llm_result.generations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Generation(text='\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side.', generation_info={'finish_reason': 'stop', 'logprobs': None}),\n",
       " Generation(text='\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side!', generation_info={'finish_reason': 'stop', 'logprobs': None})]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_result.generations[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Generation(text=\"\\n\\nThe world is a beautiful place\\nThe colors are so bright and true\\nAnd I feel so free and free\\nWhen I'm away from here\\nThe free world is waiting for me\\nAnd I'm going to enjoy it so much\\nThat I might even move to here\\nI'm so content here\\nAnd I'm so happy\\n\\nThe world is a beautiful place\\nAnd I feel so free and free\\nWhen I'm away from here\\nThe free world is waiting for me\\nAnd I'm going to enjoy it so much\\nThat I might even move to here\\nI'm so content here\", generation_info={'finish_reason': 'stop', 'logprobs': None}),\n",
       " Generation(text=\"\\n\\nA rose by the side of the road\\n\\nIs all I need to find my way\\n\\nTo the place I've been searching for\\n\\nAnd my heart is singing with joy\\n\\nWhen I look at this rose\\n\\nIt reminds me of the love I've found\\n\\nAnd I know that wherever I go\\n\\nI'll always find my rose by the side of the road.\", generation_info={'finish_reason': 'stop', 'logprobs': None})]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_result.generations[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_usage': {'total_tokens': 4202,\n",
       "  'prompt_tokens': 120,\n",
       "  'completion_tokens': 4082},\n",
       " 'model_name': 'text-ada-001'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_result.llm_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.get_num_tokens(\"你好\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'llm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m llm\u001b[39m.\u001b[39mget_num_tokens(\u001b[39m\"\u001b[39m\u001b[39mhello\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'llm' is not defined"
     ]
    }
   ],
   "source": [
    "llm.get_num_tokens(\"hello\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 如何使用 LLM 的异步 API [#](https://python.langchain.com/en/latest/modules/models/llms/examples/async_llm.html#how-to-use-the-async-api-for-llms \"此标题的永久链接\")\n",
    "\n",
    "LangChain 通过利用[asyncio](https://docs.python.org/3/library/asyncio.html)库为 LLM 提供异步支持。\n",
    "\n",
    "异步支持对于同时调用多个 LLM 特别有用，因为这些调用是网络绑定的。目前支持`OpenAI`、`PromptLayerOpenAI`和`ChatOpenAI`，但对其他 LLM 的异步支持在路线图上。`Anthropic`\n",
    "\n",
    "您可以使用该`agenerate`方法异步调用 OpenAI LLM。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import asyncio\n",
    "\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# 串行生成文本\n",
    "def generate_serially():\n",
    "    llm = OpenAI(temperature=0.9)\n",
    "    for _ in range(10):\n",
    "        # 调用生成文本的方法并传入输入文本\n",
    "        resp = llm.generate([\"Hello, how are you?\"])\n",
    "        # 打印生成的文本\n",
    "        print(resp.generations[0][0].text)\n",
    "\n",
    "\n",
    "# 异步生成文本\n",
    "async def async_generate(llm):\n",
    "    # 异步调用生成文本的方法并传入输入文本\n",
    "    resp = await llm.agenerate([\"Hello, how are you?\"])\n",
    "    # 打印生成的文本\n",
    "    print(resp.generations[0][0].text)\n",
    "\n",
    "\n",
    "# 并发生成文本\n",
    "async def generate_concurrently():\n",
    "    llm = OpenAI(temperature=0.9)\n",
    "    tasks = [async_generate(llm) for _ in range(10)]\n",
    "    # 并发执行生成文本的任务\n",
    "    await asyncio.gather(*tasks)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "I'm doing well, thank you! How about you?\n",
      "\n",
      "\n",
      "I'm doing great, thank you! How about you?\n",
      "\n",
      "\n",
      "I'm doing well, thank you. How about you?\n",
      "\n",
      "\n",
      "I'm doing well, thanks for asking. How about you?\n",
      "\n",
      "\n",
      "I'm doing well, thank you. How about you?\n",
      "\n",
      "\n",
      "I'm doing well, thank you. How about you?\n",
      "\n",
      "\n",
      "I'm doing great, thank you. How about you?\n",
      "\n",
      "\n",
      "I'm doing well, thanks for asking! How about you?\n",
      "\n",
      "\n",
      "I'm doing well, thank you. How about you?\n",
      "\n",
      "\n",
      "I'm doing well, thank you. How about you?\n",
      "\u001b[1mConcurrent executed in 3.42 seconds.\u001b[0m\n",
      "\n",
      "\n",
      "I'm doing well, thank you. How about you?\n",
      "\n",
      "\n",
      "I'm doing well, thank you for asking. How about you?\n",
      "\n",
      "\n",
      "I'm doing great, thanks for asking! How about you?\n",
      "\n",
      "\n",
      "I'm doing well, thanks for asking. How about you?\n",
      "\n",
      "\n",
      "I'm doing well, thank you! How about yourself?\n",
      "\n",
      "I'm doing well, thank you. How about you?\n",
      "\n",
      "\n",
      "I'm doing well, thank you. How about you?\n",
      "\n",
      "\n",
      "I'm doing well, thank you. How about you?\n",
      "\n",
      "\n",
      "I'm doing well, thank you. How about you?\n",
      "\n",
      "\n",
      "I'm doing great, thank you. How about you?\n",
      "\u001b[1mSerial executed in 19.44 seconds.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "s = time.perf_counter()\n",
    "# 如果在 Jupyter 之外运行此代码，请使用 asyncio.run(generate_concurrently())\n",
    "await generate_concurrently()\n",
    "elapsed = time.perf_counter() - s\n",
    "print('\\033[1m' + f\"Concurrent executed in {elapsed:0.2f} seconds.\" + '\\033[0m')\n",
    "\n",
    "s = time.perf_counter()\n",
    "generate_serially()\n",
    "elapsed = time.perf_counter() - s\n",
    "print('\\033[1m' + f\"Serial executed in {elapsed:0.2f} seconds.\" + '\\033[0m')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 如何编写自定义 LLM 包装器[#](https://python.langchain.com/en/latest/modules/models/llms/examples/custom_llm.html#how-to-write-a-custom-llm-wrapper \"此标题的永久链接\")\n",
    "\n",
    "如果您想使用自己的 LLM 或不同于 LangChain 支持的包装器，本笔记本介绍了如何创建自定义 LLM 包装器。\n",
    "\n",
    "自定义 LLM 只需要执行一件必需的事情：\n",
    "\n",
    "1. 一种`_call`接受字符串、一些可选停用词并返回字符串的方法\n",
    "    \n",
    "\n",
    "它可以实现第二个可选的东西：\n",
    "\n",
    "1. `_identifying_params`用于帮助打印此类的属性。应该返回字典。\n",
    "    \n",
    "\n",
    "让我们实现一个非常简单的自定义 LLM，它只返回输入的前 N ​​个字符。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, List, Mapping, Optional\n",
    "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
    "from langchain.llms.base import LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLLM(LLM):\n",
    "    \n",
    "    n: int\n",
    "        \n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"custom\"\n",
    "    \n",
    "    def _call(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "    ) -> str:\n",
    "        if stop is not None:\n",
    "            raise ValueError(\"stop kwargs are not permitted.\")\n",
    "        return prompt[:self.n]\n",
    "    \n",
    "    @property\n",
    "    def _identifying_params(self) -> Mapping[str, Any]:\n",
    "        \"\"\"Get the identifying parameters.\"\"\"\n",
    "        return {\"n\": self.n}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = CustomLLM(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a '"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(\"This is a foobar thing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mCustomLLM\u001b[0m\n",
      "Params: {'n': 10}\n"
     ]
    }
   ],
   "source": [
    "print(llm)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 如何（以及为什么）使用假 LLM [#](https://python.langchain.com/en/latest/modules/models/llms/examples/fake_llm.html#how-and-why-to-use-the-fake-llm \"此标题的永久链接\")\n",
    "\n",
    "我们公开了一个可用于测试的假 LLM 类。这允许您模拟对 LLM 的调用并模拟如果 LLM 以某种方式响应会发生什么。\n",
    "\n",
    "在本笔记本中，我们将介绍如何使用它。\n",
    "\n",
    "我们从在代理中使用 FakeLLM 开始。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms.fake import FakeListLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = load_tools([\"python_repl\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses=[\n",
    "    \"Action: Python REPL\\nAction Input: print(2 + 2)\",\n",
    "    \"Final Answer: 4\"\n",
    "]\n",
    "llm = FakeListLLM(responses=responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mAction: Python REPL\n",
      "Action Input: print(2 + 2)\u001b[0m4\n",
      "\n",
      "\n",
      "Observation: \u001b[36;1m\u001b[1;3m4\n",
      "\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mFinal Answer: 4\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'4'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"whats 2 + 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent.run(\"whats 1 + 3\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 如何（以及为什么）使用人工输入 LLM [#](https://python.langchain.com/en/latest/modules/models/llms/examples/human_input_llm.html#how-and-why-to-use-the-the-human-input-llm \"此标题的永久链接\")\n",
    "\n",
    "与假 LLM 类似，LangChain 提供了一个伪 LLM 类，可用于测试、调试或教育目的。这使您可以模拟对 LLM 的调用，并模拟人类在收到提示时的反应。\n",
    "\n",
    "在本笔记本中，我们将介绍如何使用它。\n",
    "\n",
    "我们首先在代理中使用 HumanInputLLM。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms.human import HumanInputLLM\n",
    "\n",
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = load_tools([\"wikipedia\"])\n",
    "llm = HumanInputLLM(prompt_func=lambda prompt: print(f\"\\n===PROMPT====\\n{prompt}\\n=====END OF PROMPT======\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\n",
      "===PROMPT====\n",
      "Answer the following questions as best you can. You have access to the following tools:\n",
      "\n",
      "Wikipedia: A wrapper around Wikipedia. Useful for when you need to answer general questions about people, places, companies, facts, historical events, or other subjects. Input should be a search query.\n",
      "\n",
      "Use the following format:\n",
      "\n",
      "Question: the input question you must answer\n",
      "Thought: you should always think about what to do\n",
      "Action: the action to take, should be one of [Wikipedia]\n",
      "Action Input: the input to the action\n",
      "Observation: the result of the action\n",
      "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
      "Thought: I now know the final answer\n",
      "Final Answer: the final answer to the original input question\n",
      "\n",
      "Begin!\n",
      "\n",
      "Question: What is 'Bocchi the Rock!'?\n",
      "Thought:\n",
      "=====END OF PROMPT======\n"
     ]
    }
   ],
   "source": [
    "agent.run(\"What is 'Bocchi the Rock!'?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ChatGPT\n",
    "\n",
    "问题：'Bocchi the Rock!'是什么？\n",
    "\n",
    "思考：我需要使用一个工具。\n",
    "\n",
    "操作：Wikipedia\n",
    "\n",
    "操作输入：Bocchi the Rock!，日本的四格漫画和动画系列。\n",
    "\n",
    "观察：页面：Bocchi the Rock!\n",
    "摘要：Bocchi the Rock!（ぼっち・ざ・ろっく！，Bocchi Za Rokku！）是一部由Aki Hamaji创作和插图的日本四格漫画系列。自2017年12月以来，它在Houbunsha的青年漫画杂志《Manga Time Kirara Max》上连载。截至2022年11月，该系列已经出版了五卷单行本。\n",
    "CloverWorks制作的动画电视系列于2022年10月至12月播出。该系列因其剧本、喜剧、角色以及对社交焦虑的描绘而受到赞扬，动画的视觉创意也备受称赞。\n",
    "\n",
    "页面：《Manga Time Kirara》\n",
    "摘要：《Manga Time Kirara》（まんがタイムきらら，Manga Taimu Kirara）是一本由Houbunsha出版的日本青年漫画杂志，主要连载四格漫画。该杂志于每月的第九天发售，最早是在2002年5月17日作为《Manga Time》的特别版首次出版。该杂志中的角色曾出现在名为《Kirara Fantasia》的跨界角色扮演游戏中。\n",
    "\n",
    "页面：《Manga Time Kirara Max》\n",
    "摘要：《Manga Time Kirara Max》（まんがタイムきららMAX）是一本由Houbunsha出版的日本四格青年漫画杂志。它是\"Kirara\"系列的第三本杂志，之前有《Manga Time Kirara》和《Manga Time Kirara Carat》。第一期于2004年9月29日发行。目前，该杂志于每月19日发行。\n",
    "\n",
    "思考：这些都不是相关的文章。\n",
    "\n",
    "操作：Wikipedia\n",
    "\n",
    "操作输入：Bocchi the Rock!，日本的四格漫画系列，由Aki Hamaji创作和插图。\n",
    "\n",
    "观察：页面：Bocchi the Rock!\n",
    "摘要：Bocchi the Rock!（ぼっち・ざ・ろっく！，Bocchi Za Rokku！）是一部由Aki Hamaji创作和插图的日本四格漫画系列。自2017年12月以来，它在Houbunsha的青年漫画杂志《Manga Time Kirara Max》上连载。截至2022年11月，该系列已经出版了五卷单行本。\n",
    "CloverWorks制作的动画电视系列于2022年10月至12月播出。该系列因其剧本、喜剧、角色以及对社交焦虑的描绘而受到赞扬，动画的视觉创意也备受称赞。\n",
    "\n",
    "思考：这次有效了。\n",
    "\n",
    "最终回答：《Bocchi the Rock!》是一部四格漫画系列和动画电视系列。该系列因其剧本、喜剧、角色以及对社交焦虑的描绘而受到赞扬，动画的视觉创意也备受称赞。\n",
    "\n",
    "完成任务。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 如何缓存 LLM 调用[#](https://python.langchain.com/en/latest/modules/models/llms/examples/llm_caching.html#how-to-cache-llm-calls \"此标题的永久链接\")\n",
    "\n",
    "此笔记本介绍了如何缓存单个 LLM 调用的结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 在内存缓存中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "from langchain.cache import InMemoryCache\n",
    "langchain.llm_cache = InMemoryCache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make the caching really obvious, lets use a slower model.\n",
    "llm = OpenAI(model_name=\"text-davinci-002\", n=2, best_of=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised APIError: The server had an error while processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 760292754efb432d1d02ccf608d2342c in your message.) {\n",
      "  \"error\": {\n",
      "    \"message\": \"The server had an error while processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 760292754efb432d1d02ccf608d2342c in your message.)\",\n",
      "    \"type\": \"server_error\",\n",
      "    \"param\": null,\n",
      "    \"code\": null\n",
      "  }\n",
      "}\n",
      " 500 {'error': {'message': 'The server had an error while processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 760292754efb432d1d02ccf608d2342c in your message.)', 'type': 'server_error', 'param': None, 'code': None}} {'Date': 'Tue, 23 May 2023 12:10:13 GMT', 'Content-Type': 'application/json', 'Content-Length': '366', 'Connection': 'keep-alive', 'access-control-allow-origin': '*', 'openai-processing-ms': '199', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'x-ratelimit-limit-requests': '3000', 'x-ratelimit-limit-tokens': '250000', 'x-ratelimit-remaining-requests': '2999', 'x-ratelimit-remaining-tokens': '248975', 'x-ratelimit-reset-requests': '20ms', 'x-ratelimit-reset-tokens': '245ms', 'x-request-id': '760292754efb432d1d02ccf608d2342c', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '7cbd31c67eba3b24-IAD', 'alt-svc': 'h3=\":443\"; ma=86400, h3-29=\":443\"; ma=86400'}.\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised APIError: The server had an error while processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 240c487f25d54bc4a196377bfbd02edc in your message.) {\n",
      "  \"error\": {\n",
      "    \"message\": \"The server had an error while processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 240c487f25d54bc4a196377bfbd02edc in your message.)\",\n",
      "    \"type\": \"server_error\",\n",
      "    \"param\": null,\n",
      "    \"code\": null\n",
      "  }\n",
      "}\n",
      " 500 {'error': {'message': 'The server had an error while processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 240c487f25d54bc4a196377bfbd02edc in your message.)', 'type': 'server_error', 'param': None, 'code': None}} {'Date': 'Tue, 23 May 2023 12:10:18 GMT', 'Content-Type': 'application/json', 'Content-Length': '366', 'Connection': 'keep-alive', 'access-control-allow-origin': '*', 'openai-processing-ms': '130', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'x-ratelimit-limit-requests': '3000', 'x-ratelimit-limit-tokens': '250000', 'x-ratelimit-remaining-requests': '2999', 'x-ratelimit-remaining-tokens': '248975', 'x-ratelimit-reset-requests': '20ms', 'x-ratelimit-reset-tokens': '245ms', 'x-request-id': '240c487f25d54bc4a196377bfbd02edc', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '7cbd31e7adb43b24-IAD', 'alt-svc': 'h3=\":443\"; ma=86400, h3-29=\":443\"; ma=86400'}.\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised APIError: The server had an error while processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 574a35c1c74646ec27dd2706c7f2133b in your message.) {\n",
      "  \"error\": {\n",
      "    \"message\": \"The server had an error while processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 574a35c1c74646ec27dd2706c7f2133b in your message.)\",\n",
      "    \"type\": \"server_error\",\n",
      "    \"param\": null,\n",
      "    \"code\": null\n",
      "  }\n",
      "}\n",
      " 500 {'error': {'message': 'The server had an error while processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 574a35c1c74646ec27dd2706c7f2133b in your message.)', 'type': 'server_error', 'param': None, 'code': None}} {'Date': 'Tue, 23 May 2023 12:10:23 GMT', 'Content-Type': 'application/json', 'Content-Length': '366', 'Connection': 'keep-alive', 'access-control-allow-origin': '*', 'openai-processing-ms': '130', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'x-ratelimit-limit-requests': '3000', 'x-ratelimit-limit-tokens': '250000', 'x-ratelimit-remaining-requests': '2999', 'x-ratelimit-remaining-tokens': '248975', 'x-ratelimit-reset-requests': '20ms', 'x-ratelimit-reset-tokens': '245ms', 'x-request-id': '574a35c1c74646ec27dd2706c7f2133b', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '7cbd3204eb903b24-IAD', 'alt-svc': 'h3=\":443\"; ma=86400, h3-29=\":443\"; ma=86400'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 15.6 ms\n",
      "Wall time: 17.3 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side!'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# The first time, it is not yet in cache, so it should take longer\n",
    "llm(\"Tell me a joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side!'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# The second time it is, so it goes faster\n",
    "llm(\"Tell me a joke\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQLite 缓存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'rm' �����ڲ����ⲿ���Ҳ���ǿ����еĳ���\n",
      "���������ļ���\n"
     ]
    }
   ],
   "source": [
    "!rm .langchain.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can do the same thing with a SQLite cache\n",
    "from langchain.cache import SQLiteCache\n",
    "langchain.llm_cache = SQLiteCache(database_path=\".langchain.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 78.1 ms\n",
      "Wall time: 2.44 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# The first time, it is not yet in cache, so it should take longer\n",
    "llm(\"Tell me a joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 997 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# The second time it is, so it goes faster\n",
    "llm(\"Tell me a joke\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redis缓存[#](https://python.langchain.com/en/latest/modules/models/llms/examples/llm_caching.html#redis-cache \"此标题的永久链接\")\n",
    "\n",
    "#### 标准缓存[#](https://python.langchain.com/en/latest/modules/models/llms/examples/llm_caching.html#standard-cache \"此标题的永久链接\")\n",
    "\n",
    "使用Redis缓存提示和响应。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can do the same thing with a Redis cache\n",
    "# (make sure your local Redis instance is running first before running this example)\n",
    "from redis import Redis\n",
    "from langchain.cache import RedisCache\n",
    "import langchain\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "langchain.llm_cache = RedisCache(redis_=Redis(host=\"192.168.3.34\",password=\"lowrisk\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make the caching really obvious, lets use a slower model.\n",
    "llm = OpenAI(model_name=\"text-davinci-002\", n=2, best_of=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 46.9 ms\n",
      "Wall time: 7.03 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n\\nWhy don't scientists trust atoms?\\nBecause they make up everything.\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# The first time, it is not yet in cache, so it should take longer\n",
    "llm(\"Tell me a joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 3.99 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# The first time, it is not yet in cache, so it should take longer\n",
    "llm(\"Tell me a joke\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 语义缓存[#](https://python.langchain.com/en/latest/modules/models/llms/examples/llm_caching.html#semantic-cache \"此标题的永久链接\")\n",
    "\n",
    "使用Redis缓存提示和响应，并根据语义相似性评估命中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.cache import RedisSemanticCache\n",
    "\n",
    "# redis 语义缓存\n",
    "langchain.llm_cache = RedisSemanticCache(\n",
    "    redis_url=\"redis://:lowrisk@192.168.3.34:6379\",\n",
    "    embedding=OpenAIEmbeddings()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# The first time, it is not yet in cache, so it should take longer\n",
    "llm(\"Tell me a joke\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CPU times: user 351 ms, sys: 156 ms, total: 507 ms  \n",
    "Wall time: 3.37 s  \n",
    "\"\\n\\nWhy don't scientists trust atoms?\\nBecause they make up everything.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# The second time, while not a direct hit, the question is semantically similar to the original question,\n",
    "# so it uses the cached result!\n",
    "llm(\"Tell me one joke\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CPU times: user 6.25 ms, sys: 2.72 ms, total: 8.97 ms  \n",
    "Wall time: 262 ms   \n",
    "\"\\n\\nWhy don't scientists trust atoms?\\nBecause they make up everything.\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPTCache [#](https://python.langchain.com/en/latest/modules/models/llms/examples/llm_caching.html#gptcache \"此标题的永久链接\")\n",
    "\n",
    "我们可以使用[GPTCache](https://github.com/zilliztech/GPTCache)进行精确匹配缓存或基于语义相似性缓存结果\n",
    "\n",
    "先从精确匹配的例子说起"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gptcache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gptcache import Cache\n",
    "from gptcache.manager.factory import manager_factory\n",
    "from gptcache.processor.pre import get_prompt\n",
    "from langchain.cache import GPTCache\n",
    "import hashlib\n",
    "\n",
    "def get_hashed_name(name):\n",
    "    return hashlib.sha256(name.encode()).hexdigest()\n",
    "\n",
    "def init_gptcache(cache_obj: Cache, llm: str):\n",
    "    hashed_llm = get_hashed_name(llm)\n",
    "    cache_obj.init(\n",
    "        pre_embedding_func=get_prompt,\n",
    "        data_manager=manager_factory(manager=\"map\", data_dir=f\"map_cache_{hashed_llm}\"),\n",
    "    )\n",
    "\n",
    "langchain.llm_cache = GPTCache(init_gptcache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 31.2 ms\n",
      "Wall time: 2.65 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nHow do you catch a cheetah? You tie him to a post!'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# The first time, it is not yet in cache, so it should take longer\n",
    "llm(\"Tell me a joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nHow do you catch a cheetah? You tie him to a post!'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# The second time it is, so it goes faster\n",
    "llm(\"Tell me a joke\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在让我们展示一个相似性缓存的例子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gptcache import Cache\n",
    "from gptcache.adapter.api import init_similar_cache\n",
    "from langchain.cache import GPTCache\n",
    "import hashlib\n",
    "\n",
    "def get_hashed_name(name):\n",
    "    return hashlib.sha256(name.encode()).hexdigest()\n",
    "\n",
    "def init_gptcache(cache_obj: Cache, llm: str):\n",
    "    hashed_llm = get_hashed_name(llm)\n",
    "    init_similar_cache(cache_obj=cache_obj, data_dir=f\"similar_cache_{hashed_llm}\")\n",
    "\n",
    "langchain.llm_cache = GPTCache(init_gptcache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\newAnaconda3\\envs\\langchain_python39\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 465/465 [00:00<00:00, 93.3kB/s]\n",
      "f:\\newAnaconda3\\envs\\langchain_python39\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Administrator\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 827/827 [00:00<00:00, 207kB/s]\n",
      "Downloading spiece.model: 100%|██████████| 760k/760k [00:00<00:00, 918kB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 1.31M/1.31M [00:01<00:00, 1.27MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 245/245 [00:00<00:00, 123kB/s]\n",
      "Downloading model.onnx: 100%|██████████| 46.9M/46.9M [00:05<00:00, 8.25MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 4.73 s\n",
      "Wall time: 4min 19s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# The first time, it is not yet in cache, so it should take longer\n",
    "llm(\"Tell me a joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1.55 s\n",
      "Wall time: 575 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# This is an exact match, so it finds it in the cache\n",
    "llm(\"Tell me a joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1.31 s\n",
      "Wall time: 355 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# This is not an exact match, but semantically within distance so it hits!\n",
    "llm(\"Tell me joke\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQLAlchemy 缓存[#](https://python.langchain.com/en/latest/modules/models/llms/examples/llm_caching.html#sqlalchemy-cache \"此标题的永久链接\")\n",
    "\n",
    "\\# You can use SQLAlchemyCache to cache with any SQL database supported by SQLAlchemy.\n",
    "\n",
    "\\# from langchain.cache import SQLAlchemyCache\n",
    "\\# from sqlalchemy import create\\_engine\n",
    "\n",
    "\\# engine = create\\_engine(\"postgresql://postgres:postgres@localhost:5432/postgres\")\n",
    "\\# langchain.llm\\_cache = SQLAlchemyCache(engine)\n",
    "\n",
    "Copy to clipboard\n",
    "\n",
    "### 自定义 SQLAlchemy 模式[#](https://python.langchain.com/en/latest/modules/models/llms/examples/llm_caching.html#custom-sqlalchemy-schemas \"此标题的永久链接\")\n",
    "\n",
    "\\# You can define your own declarative SQLAlchemyCache child class to customize the schema used for caching. For example, to support high-speed fulltext prompt indexing with Postgres, use:\n",
    "\n",
    "from sqlalchemy import Column, Integer, String, Computed, Index, Sequence\n",
    "from sqlalchemy import create\\_engine\n",
    "from sqlalchemy.ext.declarative import declarative\\_base\n",
    "from sqlalchemy\\_utils import TSVectorType\n",
    "from langchain.cache import SQLAlchemyCache\n",
    "\n",
    "Base \\= declarative\\_base()\n",
    "\n",
    "class FulltextLLMCache(Base):  \\# type: ignore\n",
    " \"\"\"Postgres table for fulltext-indexed LLM Cache\"\"\"\n",
    "\n",
    "    \\_\\_tablename\\_\\_ \\= \"llm\\_cache\\_fulltext\"\n",
    "    id \\= Column(Integer, Sequence('cache\\_id'), primary\\_key\\=True)\n",
    "    prompt \\= Column(String, nullable\\=False)\n",
    "    llm \\= Column(String, nullable\\=False)\n",
    "    idx \\= Column(Integer)\n",
    "    response \\= Column(String)\n",
    "    prompt\\_tsv \\= Column(TSVectorType(), Computed(\"to\\_tsvector('english', llm || ' ' || prompt)\", persisted\\=True))\n",
    "    \\_\\_table\\_args\\_\\_ \\= (\n",
    "        Index(\"idx\\_fulltext\\_prompt\\_tsv\", prompt\\_tsv, postgresql\\_using\\=\"gin\"),\n",
    "    )\n",
    "\n",
    "engine \\= create\\_engine(\"postgresql://postgres:postgres@localhost:5432/postgres\")\n",
    "langchain.llm\\_cache \\= SQLAlchemyCache(engine, FulltextLLMCache)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 可选缓存[#](https://python.langchain.com/en/latest/modules/models/llms/examples/llm_caching.html#optional-caching \"此标题的永久链接\")\n",
    "\n",
    "如果您愿意，您还可以关闭特定 LLM 的缓存。在下面的示例中，即使启用了全局缓存，我们也会为特定的 LLM 将其关闭"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(model_name=\"text-davinci-002\", n=2, best_of=2, cache=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1.73 s\n",
      "Wall time: 3.19 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "llm(\"Tell me a joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1.97 s\n",
      "Wall time: 1.28 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side.'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "llm(\"Tell me a joke\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 链中的可选缓存[#](https://python.langchain.com/en/latest/modules/models/llms/examples/llm_caching.html#optional-caching-in-chains \"此标题的永久链接\")\n",
    "\n",
    "您还可以关闭链中特定节点的缓存。请注意，由于某些接口，通常更容易先构建链，然后再编辑 LLM。\n",
    "\n",
    "作为示例，我们将加载一个汇总器 map-reduce 链。我们将缓存映射步骤的结果，但不会冻结合并步骤的结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter,RecursiveCharacterTextSplitter\n",
    "from langchain.chains.mapreduce import MapReduceChain\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.chains.summarize import load_summarize_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建两个模型\n",
    "llm = OpenAI(model_name=\"text-davinci-002\")\n",
    "no_cache_llm = OpenAI(model_name=\"text-davinci-002\", cache=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter()\n",
    "# text_splitter = CharacterTextSplitter()\n",
    "with open('data/文本.txt',encoding='utf-8') as f:\n",
    "    state_of_the_union = f.read()\n",
    "texts = text_splitter.split_text(state_of_the_union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1、\\t云南经济财政情况\\n云南地处西南边陲，整体经济偏弱，GDP2.45万亿，在中国省份中处于中等偏下（与陕西、辽宁、江西、重庆、广西体量近似 高于贵州），过去几年年均8%的经济增速主要靠固定资产投资拉动。固定资产投资主要来自基建+房地产行业，制造业投资慢于全国平均。优势行业是烟草、有色金属矿物、电力等。云南的预算体量2100亿，自由财力3300亿左右（未算上城投利息），过去的税收主要由烟草（占比40%）贡献。十四五期间云南依然有大规模的投资计划，集中在道路基建+水电，整体感觉对经济和税收的带动能力一般，但是债务会进一步加重（目前云南的道路修建量及密集度不及周边各省）。我们整体对经济的判断偏悲观。\\n2、\\t平台债务情况及债券到期情况\\n云南省目前归集到的发债平台的有息债务近1.23万亿，主要集中在省级平台（8477亿）和昆明市的平台（2911亿），两者占比超90%。目前归集到的发债主体债务的2020预算收入/2020自由财力/三年平均自由财力债务率在5.8/4.1/4.8倍。目前省里还有一家看起来比较大的未发债主体：有80多个县市的城镇开发公司的云南省扶贫投资开发有限公司。算上这个主体+下面城市的债务，预计全省的有息债务在2万亿左右，则2020预算收入/2020自由财力/三年平均自由财力债务率9.5/7.9/9.5倍。\\n2016年后其他县市基本发不出债来，主要依靠省级平台+昆明市级平台发债。目前云南省存续债券3300亿左右，省级2262亿，市级717亿。2020Q4后省级主体的债券基本都是净收缩的，云南康旅因为2020年到期压力较大，更早的时候债券就开始净收缩了。\\n从债券到期来看，2020、2021是云南省城投债券的到期高峰。2021年全年到期1346亿元，主要是省级平台的1021亿到期。月份分布上，相对压力较大的是3、4、5月，但整体分布较为均衡，每个月100亿左右的到期。\\n3、省级四家主要平台简单分析\\n排序上：云南建投>云南交投>云投控>云南康旅\\n云南建投：业务是建筑+保障房及棚改投资，市场认为资质最好的省属企业。2020Q3有息债务2361亿，债券不到300亿，基本都是长期的银行贷款，整体短期到期压力不大。授信结构基本都是大行授信且近些年授信持续增加。\\n云南交投：省级交通平台。2020Q3有息债务近2900亿，320亿的债券，大头是长期银行贷款。大行授信很多且未使用额度还有2600多亿，2020.02新闻是国开行放了1400多亿的隐性债务置换贷款。\\n云投控：拥有优质资产云能投（参股长江电力、华能水电等），但是整体资产业务比较杂。2020年归集有息债务2500亿，债务结构不好，直融占比近50%，2021年有522亿的债券到期，主要是直融滚续压力较大，银行贷款方面到期压力不大，整体以长期为主。\\n云南康旅：四大平台里资质最差的一个，业务偏经营性，有房地产开发、旅游、土地整理等。债务结构较差，2020Q3有息债务1328亿，归集到的1000亿债务里债券：银行贷款：非标=2：1：1，2021年有276亿的债券到期。债券结构比较杂（有CMBS、ABS、美元债、债权融资计划等）。2020H1相较于2018年末有150亿的银行授信收缩。']"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2522"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.get_num_tokens(text=texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [Document(page_content=t) for t in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = load_summarize_chain(llm=llm,chain_type=\"map_reduce\",reduce_llm = no_cache_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\nYunnan Chengtou was renamed Yunnan Kanglu in October 2020. The company's main business sectors are urban development, healthcare, tourism, and water services. Healthcare and tourism are the company's focus for the future, with support expected, but current scale is average. The urban development sector is the company's main source of income and profits at the moment, but it is also under pressure, facing difficulties with divestment, limited space for refinancing, and high financing pressure. The company needs to change its business model and integrate it with the core business. The water sector is small but has potential. The company's subsidiary, Yunnan Chengtou Real Estate Co., Ltd., is responsible for the real estate sector. Another subsidiary, Yunnan Water Co., Ltd., is the main financing and investment entity for water projects in Yunnan Province. Yunnan Investment Holding Group Co., Ltd. (Yunnan Investment) is the largest comprehensive investment entity under the Yunnan Provincial government, with controlling stakes in Yunnan Energy Investment Holding Group Co., Ltd. (Yunnan Energy) and Yunnan Iron & Steel Investment Holding Group Co., Ltd. (Yunnan Iron).\""
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\nYunnan Chengtou was renamed Yunnan Kanglu in October 2020. The company's main business sectors are urban development, healthcare, tourism, and water services. Healthcare and tourism are the company's focus for the future, but the urban development sector is the company's main source of income and profits at the moment. The water sector is small but has potential. The company's subsidiary, Yunnan Chengtou Real Estate Co., Ltd., is responsible for the real estate sector. Another subsidiary, Yunnan Water Co., Ltd., is the main financing and investment entity for water projects in Yunnan Province. Yunnan Investment Holding Group Co., Ltd. (Yunnan Investment) is the largest comprehensive investment entity under the Yunnan Provincial government, with controlling stakes in Yunnan Energy Investment Holding Group Co., Ltd. (Yunnan Energy) and Yunnan Iron & Steel Investment Holding Group Co., Ltd. (Yunnan Iron).\""
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(docs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 如何流式传输 LLM 和聊天模型响应[#](https://python.langchain.com/en/latest/modules/models/llms/examples/streaming_llm.html#how-to-stream-llm-and-chat-model-responses \"此标题的永久链接\")\n",
    "\n",
    "LangChain 为 LLM 提供流媒体支持。目前，我们支持`OpenAI`、`ChatOpenAI`和`ChatAnthropic`实现的流式处理，但对其他 LLM 实现的流式处理支持也在路线图上。要利用流式传输，请使用[`CallbackHandler`](https://github.com/hwchase17/langchain/blob/master/langchain/callbacks/base.py)实现`on_llm_new_token`. 在这个例子中，我们使用`StreamingStdOutCallbackHandler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI, ChatAnthropic\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.schema import HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "def bubble_sort(list):\n",
      "    for i in range(len(list)-1):\n",
      "        for j in range(len(list)-1-i):\n",
      "            if list[j] > list[j+1]:\n",
      "                list[j], list[j+1] = list[j+1], list[j]\n",
      "    return list\n",
      "\n",
      "list = [3,5,2,1,4]\n",
      "print(bubble_sort(list))"
     ]
    }
   ],
   "source": [
    "llm = OpenAI(streaming=True, callbacks=[StreamingStdOutCallbackHandler()], temperature=0)\n",
    "resp = llm(\"请用PYthon实现冒泡排序？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Q: What did the fish say when it hit the wall?\n",
      "A: Dam!"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LLMResult(generations=[[Generation(text='\\n\\nQ: What did the fish say when it hit the wall?\\nA: Dam!', generation_info={'finish_reason': 'stop', 'logprobs': None})]], llm_output={'token_usage': {}, 'model_name': 'text-davinci-003'})"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.generate([\"Tell me a joke.\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verse 1:\n",
      "Bubbles rising to the top\n",
      "A refreshing drink that never stops\n",
      "Clear and crisp, it's oh so pure\n",
      "Sparkling water, I can't ignore\n",
      "\n",
      "Chorus:\n",
      "Sparkling water, oh how you shine\n",
      "A taste so clean, it's simply divine\n",
      "You quench my thirst, you make me feel alive\n",
      "Sparkling water, you're my favorite vibe\n",
      "\n",
      "Verse 2:\n",
      "No sugar, no calories, just H2O\n",
      "A drink that's good for me, don't you know\n",
      "With lemon or lime, you're even better\n",
      "Sparkling water, you're my forever\n",
      "\n",
      "Chorus:\n",
      "Sparkling water, oh how you shine\n",
      "A taste so clean, it's simply divine\n",
      "You quench my thirst, you make me feel alive\n",
      "Sparkling water, you're my favorite vibe\n",
      "\n",
      "Bridge:\n",
      "You're my go-to drink, day or night\n",
      "You make me feel so light\n",
      "I'll never give you up, you're my true love\n",
      "Sparkling water, you're sent from above\n",
      "\n",
      "Chorus:\n",
      "Sparkling water, oh how you shine\n",
      "A taste so clean, it's simply divine\n",
      "You quench my thirst, you make me feel alive\n",
      "Sparkling water, you're my favorite vibe\n",
      "\n",
      "Outro:\n",
      "Sparkling water, you're the one for me\n",
      "I'll never let you go, can't you see\n",
      "You're my drink of choice, forevermore\n",
      "Sparkling water, I adore."
     ]
    }
   ],
   "source": [
    "chat = ChatOpenAI(streaming=True, callbacks=[StreamingStdOutCallbackHandler()], temperature=0)\n",
    "resp = chat([HumanMessage(content=\"Write me a song about sparkling water.\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for ChatAnthropic\n__root__\n  Did not find anthropic_api_key, please add an environment variable `ANTHROPIC_API_KEY` which contains it, or pass  `anthropic_api_key` as a named parameter. (type=value_error)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[162], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m chat \u001b[39m=\u001b[39m ChatAnthropic(streaming\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, callbacks\u001b[39m=\u001b[39;49m[StreamingStdOutCallbackHandler()], temperature\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[0;32m      2\u001b[0m resp \u001b[39m=\u001b[39m chat([HumanMessage(content\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m请介绍一下你自己\u001b[39m\u001b[39m\"\u001b[39m)])\n",
      "File \u001b[1;32mf:\\newAnaconda3\\envs\\langchain_python39\\lib\\site-packages\\pydantic\\main.py:341\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValidationError\u001b[0m: 1 validation error for ChatAnthropic\n__root__\n  Did not find anthropic_api_key, please add an environment variable `ANTHROPIC_API_KEY` which contains it, or pass  `anthropic_api_key` as a named parameter. (type=value_error)"
     ]
    }
   ],
   "source": [
    "chat = ChatAnthropic(streaming=True, callbacks=[StreamingStdOutCallbackHandler()], temperature=0)\n",
    "resp = chat([HumanMessage(content=\"请介绍一下你自己\")])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 如何跟踪令牌使用情况[#](https://python.langchain.com/en/latest/modules/models/llms/examples/token_usage_tracking.html#how-to-track-token-usage \"此标题的永久链接\")\n",
    "\n",
    "此笔记本介绍了如何跟踪特定呼叫的令牌使用情况。它目前仅针对 OpenAI API 实现。\n",
    "\n",
    "让我们首先看一个非常简单的跟踪单个 LLM 调用的令牌使用示例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.callbacks import get_openai_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(model_name=\"text-davinci-002\", n=2, best_of=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens Used: 0\n",
      "\tPrompt Tokens: 0\n",
      "\tCompletion Tokens: 0\n",
      "Successful Requests: 0\n",
      "Total Cost (USD): $0.0\n"
     ]
    }
   ],
   "source": [
    "with get_openai_callback() as cb:\n",
    "    result = llm(\"Tell me a joke\")\n",
    "    print(cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "with get_openai_callback() as cb:\n",
    "    result = llm(\"Tell me a joke\")\n",
    "    result2 = llm(\"Tell me a joke\")\n",
    "    print(cb.total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install google-search-results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 如果使用其中包含多个步骤的链或代理，它将跟踪所有这些步骤。\n",
    "\n",
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
    "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to find out who Olivia Wilde's boyfriend is and then calculate his age raised to the 0.23 power.\n",
      "Action: Search\n",
      "Action Input: \"Olivia Wilde boyfriend\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mOlivia Wilde started dating Harry Styles after ending her years-long engagement to Jason Sudeikis — see their relationship timeline.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I need to find out who Olivia Wilde's boyfriend is and then calculate his age raised to the 0.23 power.\n",
      "Action: Search\n",
      "Action Input: \"Olivia Wilde boyfriend\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mOlivia Wilde started dating Harry Styles after ending her years-long engagement to Jason Sudeikis — see their relationship timeline.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I need to find out who Olivia Wilde's boyfriend is and then calculate his age raised to the 0.23 power.\n",
      "Action: Search\n",
      "Action Input: \"Olivia Wilde boyfriend\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mOlivia Wilde started dating Harry Styles after ending her years-long engagement to Jason Sudeikis — see their relationship timeline.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I need to find out who Olivia Wilde's boyfriend is and then calculate his age raised to the 0.23 power.\n",
      "Action: Search\n",
      "Action Input: \"Olivia Wilde boyfriend\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mOlivia Wilde started dating Harry Styles after ending her years-long engagement to Jason Sudeikis — see their relationship timeline.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I need to find out who Olivia Wilde's boyfriend is and then calculate his age raised to the 0.23 power.\n",
      "Action: Search\n",
      "Action Input: \"Olivia Wilde boyfriend\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mOlivia Wilde started dating Harry Styles after ending her years-long engagement to Jason Sudeikis — see their relationship timeline.\u001b[0m\n",
      "Thought:"
     ]
    },
    {
     "ename": "InvalidArgument",
     "evalue": "[ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Got invalid dimensions for input: token_type_ids for the following indices\n index: 1 Got: 525 Expected: 512\n Please fix either the inputs or the model.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgument\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[177], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mwith\u001b[39;00m get_openai_callback() \u001b[39mas\u001b[39;00m cb:\n\u001b[1;32m----> 2\u001b[0m     response \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39;49mrun(\u001b[39m\"\u001b[39;49m\u001b[39mWho is Olivia Wilde\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39ms boyfriend? What is his current age raised to the 0.23 power?\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      3\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTotal Tokens: \u001b[39m\u001b[39m{\u001b[39;00mcb\u001b[39m.\u001b[39mtotal_tokens\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPrompt Tokens: \u001b[39m\u001b[39m{\u001b[39;00mcb\u001b[39m.\u001b[39mprompt_tokens\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mf:\\newAnaconda3\\envs\\langchain_python39\\lib\\site-packages\\langchain\\chains\\base.py:236\u001b[0m, in \u001b[0;36mChain.run\u001b[1;34m(self, callbacks, *args, **kwargs)\u001b[0m\n\u001b[0;32m    234\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    235\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m`run` supports only one positional argument.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 236\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(args[\u001b[39m0\u001b[39;49m], callbacks\u001b[39m=\u001b[39;49mcallbacks)[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_keys[\u001b[39m0\u001b[39m]]\n\u001b[0;32m    238\u001b[0m \u001b[39mif\u001b[39;00m kwargs \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m args:\n\u001b[0;32m    239\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m(kwargs, callbacks\u001b[39m=\u001b[39mcallbacks)[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_keys[\u001b[39m0\u001b[39m]]\n",
      "File \u001b[1;32mf:\\newAnaconda3\\envs\\langchain_python39\\lib\\site-packages\\langchain\\chains\\base.py:140\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    139\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 140\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m    141\u001b[0m run_manager\u001b[39m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m    142\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(inputs, outputs, return_only_outputs)\n",
      "File \u001b[1;32mf:\\newAnaconda3\\envs\\langchain_python39\\lib\\site-packages\\langchain\\chains\\base.py:134\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks)\u001b[0m\n\u001b[0;32m    128\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[0;32m    129\u001b[0m     {\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m},\n\u001b[0;32m    130\u001b[0m     inputs,\n\u001b[0;32m    131\u001b[0m )\n\u001b[0;32m    132\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    133\u001b[0m     outputs \u001b[39m=\u001b[39m (\n\u001b[1;32m--> 134\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs, run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[0;32m    135\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    136\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(inputs)\n\u001b[0;32m    137\u001b[0m     )\n\u001b[0;32m    138\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    139\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[1;32mf:\\newAnaconda3\\envs\\langchain_python39\\lib\\site-packages\\langchain\\agents\\agent.py:947\u001b[0m, in \u001b[0;36mAgentExecutor._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m    945\u001b[0m \u001b[39m# We now enter the agent loop (until it returns something).\u001b[39;00m\n\u001b[0;32m    946\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_continue(iterations, time_elapsed):\n\u001b[1;32m--> 947\u001b[0m     next_step_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_take_next_step(\n\u001b[0;32m    948\u001b[0m         name_to_tool_map,\n\u001b[0;32m    949\u001b[0m         color_mapping,\n\u001b[0;32m    950\u001b[0m         inputs,\n\u001b[0;32m    951\u001b[0m         intermediate_steps,\n\u001b[0;32m    952\u001b[0m         run_manager\u001b[39m=\u001b[39;49mrun_manager,\n\u001b[0;32m    953\u001b[0m     )\n\u001b[0;32m    954\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(next_step_output, AgentFinish):\n\u001b[0;32m    955\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_return(\n\u001b[0;32m    956\u001b[0m             next_step_output, intermediate_steps, run_manager\u001b[39m=\u001b[39mrun_manager\n\u001b[0;32m    957\u001b[0m         )\n",
      "File \u001b[1;32mf:\\newAnaconda3\\envs\\langchain_python39\\lib\\site-packages\\langchain\\agents\\agent.py:762\u001b[0m, in \u001b[0;36mAgentExecutor._take_next_step\u001b[1;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[0;32m    756\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Take a single step in the thought-action-observation loop.\u001b[39;00m\n\u001b[0;32m    757\u001b[0m \n\u001b[0;32m    758\u001b[0m \u001b[39mOverride this to take control of how the agent makes and acts on choices.\u001b[39;00m\n\u001b[0;32m    759\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    760\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    761\u001b[0m     \u001b[39m# Call the LLM to see what to do.\u001b[39;00m\n\u001b[1;32m--> 762\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39magent\u001b[39m.\u001b[39mplan(\n\u001b[0;32m    763\u001b[0m         intermediate_steps,\n\u001b[0;32m    764\u001b[0m         callbacks\u001b[39m=\u001b[39mrun_manager\u001b[39m.\u001b[39mget_child() \u001b[39mif\u001b[39;00m run_manager \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    765\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39minputs,\n\u001b[0;32m    766\u001b[0m     )\n\u001b[0;32m    767\u001b[0m \u001b[39mexcept\u001b[39;00m OutputParserException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    768\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_parsing_errors, \u001b[39mbool\u001b[39m):\n",
      "File \u001b[1;32mf:\\newAnaconda3\\envs\\langchain_python39\\lib\\site-packages\\langchain\\agents\\agent.py:443\u001b[0m, in \u001b[0;36mAgent.plan\u001b[1;34m(self, intermediate_steps, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    431\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Given input, decided what to do.\u001b[39;00m\n\u001b[0;32m    432\u001b[0m \n\u001b[0;32m    433\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    440\u001b[0m \u001b[39m    Action specifying what tool to use.\u001b[39;00m\n\u001b[0;32m    441\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    442\u001b[0m full_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_full_inputs(intermediate_steps, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 443\u001b[0m full_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mllm_chain\u001b[39m.\u001b[39mpredict(callbacks\u001b[39m=\u001b[39mcallbacks, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfull_inputs)\n\u001b[0;32m    444\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_parser\u001b[39m.\u001b[39mparse(full_output)\n",
      "File \u001b[1;32mf:\\newAnaconda3\\envs\\langchain_python39\\lib\\site-packages\\langchain\\chains\\llm.py:213\u001b[0m, in \u001b[0;36mLLMChain.predict\u001b[1;34m(self, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, callbacks: Callbacks \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[0;32m    199\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Format prompt with kwargs and pass to LLM.\u001b[39;00m\n\u001b[0;32m    200\u001b[0m \n\u001b[0;32m    201\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[39m            completion = llm.predict(adjective=\"funny\")\u001b[39;00m\n\u001b[0;32m    212\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 213\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(kwargs, callbacks\u001b[39m=\u001b[39;49mcallbacks)[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_key]\n",
      "File \u001b[1;32mf:\\newAnaconda3\\envs\\langchain_python39\\lib\\site-packages\\langchain\\chains\\base.py:140\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    139\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 140\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m    141\u001b[0m run_manager\u001b[39m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m    142\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(inputs, outputs, return_only_outputs)\n",
      "File \u001b[1;32mf:\\newAnaconda3\\envs\\langchain_python39\\lib\\site-packages\\langchain\\chains\\base.py:134\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks)\u001b[0m\n\u001b[0;32m    128\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[0;32m    129\u001b[0m     {\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m},\n\u001b[0;32m    130\u001b[0m     inputs,\n\u001b[0;32m    131\u001b[0m )\n\u001b[0;32m    132\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    133\u001b[0m     outputs \u001b[39m=\u001b[39m (\n\u001b[1;32m--> 134\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs, run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[0;32m    135\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    136\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(inputs)\n\u001b[0;32m    137\u001b[0m     )\n\u001b[0;32m    138\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    139\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[1;32mf:\\newAnaconda3\\envs\\langchain_python39\\lib\\site-packages\\langchain\\chains\\llm.py:69\u001b[0m, in \u001b[0;36mLLMChain._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call\u001b[39m(\n\u001b[0;32m     65\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m     66\u001b[0m     inputs: Dict[\u001b[39mstr\u001b[39m, Any],\n\u001b[0;32m     67\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m     68\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[1;32m---> 69\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate([inputs], run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[0;32m     70\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreate_outputs(response)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mf:\\newAnaconda3\\envs\\langchain_python39\\lib\\site-packages\\langchain\\chains\\llm.py:79\u001b[0m, in \u001b[0;36mLLMChain.generate\u001b[1;34m(self, input_list, run_manager)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Generate LLM result from inputs.\"\"\"\u001b[39;00m\n\u001b[0;32m     78\u001b[0m prompts, stop \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_prompts(input_list, run_manager\u001b[39m=\u001b[39mrun_manager)\n\u001b[1;32m---> 79\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mllm\u001b[39m.\u001b[39;49mgenerate_prompt(\n\u001b[0;32m     80\u001b[0m     prompts, stop, callbacks\u001b[39m=\u001b[39;49mrun_manager\u001b[39m.\u001b[39;49mget_child() \u001b[39mif\u001b[39;49;00m run_manager \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m\n\u001b[0;32m     81\u001b[0m )\n",
      "File \u001b[1;32mf:\\newAnaconda3\\envs\\langchain_python39\\lib\\site-packages\\langchain\\llms\\base.py:134\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_prompt\u001b[39m(\n\u001b[0;32m    128\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    129\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m    130\u001b[0m     stop: Optional[List[\u001b[39mstr\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    131\u001b[0m     callbacks: Callbacks \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    132\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m LLMResult:\n\u001b[0;32m    133\u001b[0m     prompt_strings \u001b[39m=\u001b[39m [p\u001b[39m.\u001b[39mto_string() \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m prompts]\n\u001b[1;32m--> 134\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate(prompt_strings, stop\u001b[39m=\u001b[39;49mstop, callbacks\u001b[39m=\u001b[39;49mcallbacks)\n",
      "File \u001b[1;32mf:\\newAnaconda3\\envs\\langchain_python39\\lib\\site-packages\\langchain\\llms\\base.py:166\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[1;34m(self, prompts, stop, callbacks)\u001b[0m\n\u001b[0;32m    159\u001b[0m params \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdict()\n\u001b[0;32m    160\u001b[0m params[\u001b[39m\"\u001b[39m\u001b[39mstop\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m stop\n\u001b[0;32m    161\u001b[0m (\n\u001b[0;32m    162\u001b[0m     existing_prompts,\n\u001b[0;32m    163\u001b[0m     llm_string,\n\u001b[0;32m    164\u001b[0m     missing_prompt_idxs,\n\u001b[0;32m    165\u001b[0m     missing_prompts,\n\u001b[1;32m--> 166\u001b[0m ) \u001b[39m=\u001b[39m get_prompts(params, prompts)\n\u001b[0;32m    167\u001b[0m disregard_cache \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcache \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcache\n\u001b[0;32m    168\u001b[0m callback_manager \u001b[39m=\u001b[39m CallbackManager\u001b[39m.\u001b[39mconfigure(\n\u001b[0;32m    169\u001b[0m     callbacks, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallbacks, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose\n\u001b[0;32m    170\u001b[0m )\n",
      "File \u001b[1;32mf:\\newAnaconda3\\envs\\langchain_python39\\lib\\site-packages\\langchain\\llms\\base.py:46\u001b[0m, in \u001b[0;36mget_prompts\u001b[1;34m(params, prompts)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[39mfor\u001b[39;00m i, prompt \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(prompts):\n\u001b[0;32m     45\u001b[0m     \u001b[39mif\u001b[39;00m langchain\u001b[39m.\u001b[39mllm_cache \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m---> 46\u001b[0m         cache_val \u001b[39m=\u001b[39m langchain\u001b[39m.\u001b[39;49mllm_cache\u001b[39m.\u001b[39;49mlookup(prompt, llm_string)\n\u001b[0;32m     47\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(cache_val, \u001b[39mlist\u001b[39m):\n\u001b[0;32m     48\u001b[0m             existing_prompts[i] \u001b[39m=\u001b[39m cache_val\n",
      "File \u001b[1;32mf:\\newAnaconda3\\envs\\langchain_python39\\lib\\site-packages\\langchain\\cache.py:365\u001b[0m, in \u001b[0;36mGPTCache.lookup\u001b[1;34m(self, prompt, llm_string)\u001b[0m\n\u001b[0;32m    363\u001b[0m \u001b[39mif\u001b[39;00m _gptcache \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    364\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 365\u001b[0m res \u001b[39m=\u001b[39m get(prompt, cache_obj\u001b[39m=\u001b[39;49m_gptcache)\n\u001b[0;32m    366\u001b[0m \u001b[39mif\u001b[39;00m res:\n\u001b[0;32m    367\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[0;32m    368\u001b[0m         Generation(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mgeneration_dict) \u001b[39mfor\u001b[39;00m generation_dict \u001b[39min\u001b[39;00m json\u001b[39m.\u001b[39mloads(res)\n\u001b[0;32m    369\u001b[0m     ]\n",
      "File \u001b[1;32mf:\\newAnaconda3\\envs\\langchain_python39\\lib\\site-packages\\gptcache\\adapter\\api.py:113\u001b[0m, in \u001b[0;36mget\u001b[1;34m(prompt, **kwargs)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(prompt: \u001b[39mstr\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m     95\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"get api, get the cache data according to the `prompt`\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \u001b[39m    Please make sure that the `pre_embedding_func` param is `get_prompt` when initializing the cache\u001b[39;00m\n\u001b[0;32m     97\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[39m            print(get(\"hello\"))\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 113\u001b[0m     res \u001b[39m=\u001b[39m adapt(\n\u001b[0;32m    114\u001b[0m         _llm_handle_none,\n\u001b[0;32m    115\u001b[0m         _cache_data_converter,\n\u001b[0;32m    116\u001b[0m         _update_cache_callback_none,\n\u001b[0;32m    117\u001b[0m         prompt\u001b[39m=\u001b[39mprompt,\n\u001b[0;32m    118\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    119\u001b[0m     )\n\u001b[0;32m    120\u001b[0m     \u001b[39mreturn\u001b[39;00m res\n",
      "File \u001b[1;32mf:\\newAnaconda3\\envs\\langchain_python39\\lib\\site-packages\\gptcache\\adapter\\adapter.py:52\u001b[0m, in \u001b[0;36madapt\u001b[1;34m(llm_handler, cache_data_convert, update_cache_callback, *args, **kwargs)\u001b[0m\n\u001b[0;32m     49\u001b[0m     pre_embedding_data \u001b[39m=\u001b[39m pre_embedding_res\n\u001b[0;32m     51\u001b[0m \u001b[39mif\u001b[39;00m cache_enable:\n\u001b[1;32m---> 52\u001b[0m     embedding_data \u001b[39m=\u001b[39m time_cal(\n\u001b[0;32m     53\u001b[0m         chat_cache\u001b[39m.\u001b[39;49membedding_func,\n\u001b[0;32m     54\u001b[0m         func_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39membedding\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     55\u001b[0m         report_func\u001b[39m=\u001b[39;49mchat_cache\u001b[39m.\u001b[39;49mreport\u001b[39m.\u001b[39;49membedding,\n\u001b[0;32m     56\u001b[0m     )(pre_embedding_data, extra_param\u001b[39m=\u001b[39;49mcontext\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39membedding_func\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m))\n\u001b[0;32m     57\u001b[0m \u001b[39mif\u001b[39;00m cache_enable \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m cache_skip:\n\u001b[0;32m     58\u001b[0m     cache_data_list \u001b[39m=\u001b[39m time_cal(\n\u001b[0;32m     59\u001b[0m         chat_cache\u001b[39m.\u001b[39mdata_manager\u001b[39m.\u001b[39msearch,\n\u001b[0;32m     60\u001b[0m         func_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msearch\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     67\u001b[0m         \u001b[39melse\u001b[39;00m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mtop_k\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m),\n\u001b[0;32m     68\u001b[0m     )\n",
      "File \u001b[1;32mf:\\newAnaconda3\\envs\\langchain_python39\\lib\\site-packages\\gptcache\\utils\\time.py:9\u001b[0m, in \u001b[0;36mtime_cal.<locals>.inner\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minner\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m      8\u001b[0m     time_start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m----> 9\u001b[0m     res \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     10\u001b[0m     delta_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m time_start\n\u001b[0;32m     11\u001b[0m     \u001b[39mif\u001b[39;00m cache\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mlog_time_func:\n",
      "File \u001b[1;32mf:\\newAnaconda3\\envs\\langchain_python39\\lib\\site-packages\\gptcache\\embedding\\onnx.py:59\u001b[0m, in \u001b[0;36mOnnx.to_embeddings\u001b[1;34m(self, data, **_)\u001b[0m\n\u001b[0;32m     51\u001b[0m encoded_text \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer\u001b[39m.\u001b[39mencode_plus(data, padding\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmax_length\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     53\u001b[0m ort_inputs \u001b[39m=\u001b[39m {\n\u001b[0;32m     54\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m: np\u001b[39m.\u001b[39marray(encoded_text[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m])\u001b[39m.\u001b[39mastype(\u001b[39m\"\u001b[39m\u001b[39mint64\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mreshape(\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m),\n\u001b[0;32m     55\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m\"\u001b[39m: np\u001b[39m.\u001b[39marray(encoded_text[\u001b[39m\"\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m\"\u001b[39m])\u001b[39m.\u001b[39mastype(\u001b[39m\"\u001b[39m\u001b[39mint64\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mreshape(\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m),\n\u001b[0;32m     56\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtoken_type_ids\u001b[39m\u001b[39m\"\u001b[39m: np\u001b[39m.\u001b[39marray(encoded_text[\u001b[39m\"\u001b[39m\u001b[39mtoken_type_ids\u001b[39m\u001b[39m\"\u001b[39m])\u001b[39m.\u001b[39mastype(\u001b[39m\"\u001b[39m\u001b[39mint64\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mreshape(\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m),\n\u001b[0;32m     57\u001b[0m }\n\u001b[1;32m---> 59\u001b[0m ort_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mort_session\u001b[39m.\u001b[39;49mrun(\u001b[39mNone\u001b[39;49;00m, ort_inputs)\n\u001b[0;32m     60\u001b[0m ort_feat \u001b[39m=\u001b[39m ort_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m     61\u001b[0m emb \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpost_proc(ort_feat, ort_inputs[\u001b[39m\"\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "File \u001b[1;32mf:\\newAnaconda3\\envs\\langchain_python39\\lib\\site-packages\\onnxruntime\\capi\\onnxruntime_inference_collection.py:217\u001b[0m, in \u001b[0;36mSession.run\u001b[1;34m(self, output_names, input_feed, run_options)\u001b[0m\n\u001b[0;32m    215\u001b[0m     output_names \u001b[39m=\u001b[39m [output\u001b[39m.\u001b[39mname \u001b[39mfor\u001b[39;00m output \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_outputs_meta]\n\u001b[0;32m    216\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 217\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sess\u001b[39m.\u001b[39;49mrun(output_names, input_feed, run_options)\n\u001b[0;32m    218\u001b[0m \u001b[39mexcept\u001b[39;00m C\u001b[39m.\u001b[39mEPFail \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m    219\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_enable_fallback:\n",
      "\u001b[1;31mInvalidArgument\u001b[0m: [ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Got invalid dimensions for input: token_type_ids for the following indices\n index: 1 Got: 525 Expected: 512\n Please fix either the inputs or the model."
     ]
    }
   ],
   "source": [
    "with get_openai_callback() as cb:\n",
    "    response = agent.run(\"Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?\")\n",
    "    print(f\"Total Tokens: {cb.total_tokens}\")\n",
    "    print(f\"Prompt Tokens: {cb.prompt_tokens}\")\n",
    "    print(f\"Completion Tokens: {cb.completion_tokens}\")\n",
    "    print(f\"Total Cost (USD): ${cb.total_cost}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 聊天模型chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"J'aime programmer.\", additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat([HumanMessage(content=\"Translate this sentence from English to French. I love programming.\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"J'adore la programmation.\", additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant that translates English to French.\"),\n",
    "    HumanMessage(content=\"I love programming.\")\n",
    "]\n",
    "chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMResult(generations=[[ChatGeneration(text='我喜欢编程。', generation_info=None, message=AIMessage(content='我喜欢编程。', additional_kwargs={}, example=False))], [ChatGeneration(text='我喜欢人工智能。', generation_info=None, message=AIMessage(content='我喜欢人工智能。', additional_kwargs={}, example=False))]], llm_output={'token_usage': {'prompt_tokens': 57, 'completion_tokens': 19, 'total_tokens': 76}, 'model_name': 'gpt-3.5-turbo'})"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_messages = [\n",
    "    [\n",
    "        SystemMessage(content=\"You are a helpful assistant that translates English to Chinese.\"),\n",
    "        HumanMessage(content=\"I love programming.\")\n",
    "    ],\n",
    "    [\n",
    "        SystemMessage(content=\"You are a helpful assistant that translates English to Chinese.\"),\n",
    "        HumanMessage(content=\"I love artificial intelligence.\")\n",
    "    ],\n",
    "]\n",
    "result = chat.generate(batch_messages)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_usage': {'prompt_tokens': 57,\n",
       "  'completion_tokens': 19,\n",
       "  'total_tokens': 76},\n",
       " 'model_name': 'gpt-3.5-turbo'}"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.llm_output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 提示模板[#](https://python.langchain.com/en/latest/modules/models/chat/getting_started.html#prompttemplates \"此标题的永久链接\")\n",
    "\n",
    "您可以通过使用`MessagePromptTemplate`. 您可以`ChatPromptTemplate`从一个或多个构建一个`MessagePromptTemplates`。您可以使用`ChatPromptTemplate`'s `format_prompt`– 这会返回一个`PromptValue`，您可以将其转换为字符串或 Message 对象，具体取决于您是要使用格式化值作为 llm 还是聊天模型的输入。\n",
    "\n",
    "为方便起见，`from_template`模板上公开了一个方法。如果您要使用此模板，它会是这样的："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "template=\"You are a helpful assistant that translates {input_language} to {output_language}.\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "human_template=\"{text}\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langchain.chat_models.openai:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='我喜欢编程。', additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "\n",
    "# get a chat completion from the formatted messages\n",
    "chat(chat_prompt.format_prompt(input_language=\"English\", output_language=\"Chinese\", text=\"I love programming.\").to_messages())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=PromptTemplate(\n",
    "    template=\"You are a helpful assistant that translates {input_language} to {output_language}.\",\n",
    "    input_variables=[\"input_language\", \"output_language\"],\n",
    ")\n",
    "system_message_prompt = SystemMessagePromptTemplate(prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"J'adore la programmation.\""
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = LLMChain(llm=chat, prompt=chat_prompt)\n",
    "chain.run(input_language=\"English\", output_language=\"French\", text=\"I love programming.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verse 1:\n",
      "Bubbles rising to the top\n",
      "A refreshing drink that never stops\n",
      "Clear and crisp, it's oh so pure\n",
      "Sparkling water, I can't ignore\n",
      "\n",
      "Chorus:\n",
      "Sparkling water, oh how you shine\n",
      "A taste so clean, it's simply divine\n",
      "You quench my thirst, you make me feel alive\n",
      "Sparkling water, you're my favorite vibe\n",
      "\n",
      "Verse 2:\n",
      "No sugar, no calories, just H2O\n",
      "A drink that's good for me, don't you know\n",
      "With lemon or lime, you're even better\n",
      "Sparkling water, you're my forever\n",
      "\n",
      "Chorus:\n",
      "Sparkling water, oh how you shine\n",
      "A taste so clean, it's simply divine\n",
      "You quench my thirst, you make me feel alive\n",
      "Sparkling water, you're my favorite vibe\n",
      "\n",
      "Bridge:\n",
      "You're my go-to drink, day or night\n",
      "You make me feel so light\n",
      "I'll never give you up, you're my true love\n",
      "Sparkling water, you're sent from above\n",
      "\n",
      "Chorus:\n",
      "Sparkling water, oh how you shine\n",
      "A taste so clean, it's simply divine\n",
      "You quench my thirst, you make me feel alive\n",
      "Sparkling water, you're my favorite vibe\n",
      "\n",
      "Outro:\n",
      "Sparkling water, you're the one for me\n",
      "I'll never let you go, can't you see\n",
      "You're my drink of choice, forevermore\n",
      "Sparkling water, I adore."
     ]
    }
   ],
   "source": [
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "chat = ChatOpenAI(streaming=True, callbacks=[StreamingStdOutCallbackHandler()], temperature=0)\n",
    "resp = chat([HumanMessage(content=\"Write me a song about sparkling water.\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'rm' �����ڲ����ⲿ���Ҳ���ǿ����еĳ���\n",
      "���������ļ���\n"
     ]
    }
   ],
   "source": [
    "!rm .langchain.db sqlite.db"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
