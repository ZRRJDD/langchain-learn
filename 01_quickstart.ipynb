{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 环境配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# 加载.env\n",
    "load_dotenv('.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置openai api key\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Language Model Application: LLMs（构建语言模型应用程序：LLMs）\n",
    "既然我们已经安装了LangChain并设置好了环境，那么我们就可以开始构建我们的语言模型应用程序。\n",
    "\n",
    "LangChain提供了许多模块，可用于构建语言模型应用程序。这些模块可以组合在一起创建更复杂的应用程序，也可以单独使用于简单的应用。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLMs: Get predictions from a language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Rainbow Toes.\n"
     ]
    }
   ],
   "source": [
    "text = \"What would be a good company name for a company that makes colorful socks?\"\n",
    "print(llm(text))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Templates: Manage prompts for LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is a good name for a company that makes colorful socks?'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = PromptTemplate(input_variables=['product'],template=\"What is a good name for a company that makes {product}?\")\n",
    "prompt.format(product=\"colorful socks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chains: Combine LLMs and prompts in multi-step workflows(在多步骤工作流中结合LLMs和提示)\n",
    "\n",
    "到目前为止，我们只是单独使用了PromptTemplate和LLM原语。但实际应用程序不仅仅是一个原语，而是它们的组合。\n",
    "\n",
    "在LangChain中，链由链接组成，可以是像LLMs这样的基本元素或其他链。\n",
    "\n",
    "最核心的链类型是LLMChain，它由PromptTemplate和LLM组成。\n",
    "\n",
    "扩展之前的示例，我们可以构建一个LLMChain来接收用户输入、使用PromptTemplate格式化输入，并将格式化后的响应传递给LLM。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(openai_api_key=OPENAI_API_KEY,temperature=0.9)\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=['product'],\n",
    "    template=\"What is a good name for a company that makes {product}?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = LLMChain(llm=llm,prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nSocksational Colors.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"colorful socks\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agents: Dynamically Call Chains Based on User Input(根据用户输入动态调用链)\n",
    "\n",
    "到目前为止，我们看过的链条都是按照预定顺序运行的。\n",
    "\n",
    "现在代理不再这样做：它们使用LLM来确定采取哪些操作以及以什么顺序进行。一个动作可以是使用工具并观察其输出，或者返回给用户。\n",
    "\n",
    "如果正确使用代理，它们可以非常强大。在本教程中，我们将向您展示如何通过最简单、最高级别的API轻松地使用代理。\n",
    "\n",
    "为了加载代理程序，您应该了解以下概念：\n",
    "\n",
    "- 工具(Tool)：执行特定职责的函数。这可以是像Google搜索、数据库查找、Python REPL和其他链条之类的东西。工具的接口目前是期望有一个字符串作为输入，并且有一个字符串作为输出的函数。\n",
    "\n",
    "- LLM：支持代理程序的语言模型。\n",
    "\n",
    "- 代理程序(Agent)：要使用的代理程序。这应该是引用支持代理类别（class） 的字符串。因为本笔记本专注于最简单、最高级别API 的使用方式，所以只涵盖了标准支持代理程序（standard supported agents） 的使用方法 。如果您想实现自定义  代码，请参阅自定义代码文档 （即将推出） 。\n",
    "\n",
    "Agents: 支持Agent及其规格列表，请参见此处\n",
    "\n",
    "Tools: 预定义工具及其规格列表，请参见此处"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install google-search-results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置serp api key\n",
    "os.environ['SERPAPI_API_KEY'] = os.getenv('SERPAPI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools,initialize_agent,AgentType\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to find the temperature in Fahrenheit and then calculate the 0.23 power of that number.\n",
      "Action: Search\n",
      "Action Input: \"Beijing temperature yesterday in Fahrenheit\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mBeijing Temperature Yesterday. Maximum temperature yesterday: 65 °F (at 2:00 am) Minimum temperature yesterday: 57 °F (at 8:00 pm)\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I need to calculate the 0.23 power of 65\n",
      "Action: Calculator\n",
      "Action Input: 65^0.23\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 2.6119813521152198\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: 2.6119813521152198\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.6119813521152198'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 首先，让我们加载语言模型来控制代理。\n",
    "llm = OpenAI(temperature=0)\n",
    "# 接下来，让我们加载一些需要使用的工具。请注意，“llm-math” 工具使用了 LLM，因此我们需要传递该参数。\n",
    "tools = load_tools([\"serpapi\",\"llm-math\"],llm=llm)\n",
    "# 最后，让我们使用工具、语言模型和所需的代理类型来初始化一个代理。verbose 设置是否显示详细信息\n",
    "agent = initialize_agent(tools=tools,llm=llm,agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,verbose=True)\n",
    "\n",
    "# 昨天旧金山的最高气温是多少华氏度？这个数字的0.23次方是多少？\n",
    "# agent.run(\"What was the high temperature in SF yesterday in Fahrenheit? What is that number raised to the .023 power?\")\n",
    "agent.run(\"昨天北京的最高气温是多少华氏度？这个数字的0.23次方是多少？\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory: Add State to Chains and Agents（为链和代理添加状态）\n",
    "\n",
    "到目前为止，我们所使用的所有链和代理都是无状态的。但通常情况下，您可能希望某个链或代理具有一些“记忆”概念，以便它可以记住有关其先前交互的信息。最清晰和简单的例子就是设计聊天机器人 - 您希望它记住以前的消息，以便可以利用上下文进行更好的对话。这将是一种“短期记忆”。在更复杂的方面，您可以想象一个链/代理随着时间推移记住重要信息 - 这将是一种“长期记忆”的形式。有关后者更具体的想法，请参见这篇精彩论文。\n",
    "\n",
    "LangChain提供了几个专门为此目的创建的链。本笔记本演示如何使用其中之一（ConversationChain）来实现两种不同类型的内存。\n",
    "\n",
    "默认情况下，ConversationChain具有一种简单类型的内存，它会记录所有先前输入/输出并将其添加到传递给它们上下文中。让我们看看如何使用此链（设置verbose=True以便查看提示）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: 你好\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' 你好！很高兴见到你！我是一个智能AI，我可以回答你的问题，或者我们可以聊聊天？'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import OpenAI,ConversationChain\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "conversation = ConversationChain(llm=llm,verbose=True)\n",
    "\n",
    "conversation.predict(input=\"你好\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: 你好\n",
      "AI:  你好！很高兴见到你！我是一个智能AI，我可以回答你的问题，或者我们可以聊聊天？\n",
      "Human: 我的名字叫：ZRRJDD\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' 嗨，ZRRJDD！很高兴认识你！你有什么想聊的？'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"我的名字叫：ZRRJDD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: 你好\n",
      "AI:  你好！很高兴见到你！我是一个智能AI，我可以回答你的问题，或者我们可以聊聊天？\n",
      "Human: 我的名字叫：ZRRJDD\n",
      "AI:  嗨，ZRRJDD！很高兴认识你！你有什么想聊的？\n",
      "Human: 你知道我的名字吗？\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' 不，我不知道你的名字。但是我很乐意认识你！'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"你知道我的名字吗？\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Language Model Application: Chat Models（构建语言模型应用程序：聊天模型）\n",
    "同样，您可以使用聊天模型代替LLMs。 聊天模型是语言模型的一种变体。虽然聊天模型在底层使用语言模型，但它们公开的接口略有不同：它们公开了一个“输入为聊天消息，输出也为聊天消息”的接口，而不是“文本输入、文本输出”的API。\n",
    "\n",
    "由于聊天模型API相对较新，因此我们仍在摸索正确的抽象方法。\n",
    "\n",
    "## Get Message Completions from a Chat Model（从聊天模型中获取消息完成度）\n",
    "您可以通过向聊天模型传递一个或多个消息来获取聊天完成度。响应将是一条消息。目前在LangChain中支持的消息类型有AIMessage、HumanMessage、SystemMessage和ChatMessage——ChatMessage接受任意角色参数。大多数情况下，您只需要处理HumanMessage、AIMessage和SystemMessage。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import AIMessage,HumanMessage,SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='我喜欢编程。', additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 您可以通过传入单个消息来获得完成。\n",
    "chat([HumanMessage(content=\"Translate this sentence from English to Chinese. I love programming.\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='我喜欢编程。', additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 您还可以为OpenAI的gpt-3.5-turbo和gpt-4模型传入多个消息。\n",
    "messages =[\n",
    "    SystemMessage(content=\"You are a helpful assistant that translates English to Chinese.\"),\n",
    "    HumanMessage(content=\"I love programming.\")\n",
    "]\n",
    "chat(messages=messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('generations', [[ChatGeneration(text='我喜欢编程。', generation_info=None, message=AIMessage(content='我喜欢编程。', additional_kwargs={}, example=False))], [ChatGeneration(text='我喜欢人工智能。', generation_info=None, message=AIMessage(content='我喜欢人工智能。', additional_kwargs={}, example=False))]])\n",
      "('llm_output', {'token_usage': {'prompt_tokens': 57, 'completion_tokens': 19, 'total_tokens': 76}, 'model_name': 'gpt-3.5-turbo'})\n"
     ]
    }
   ],
   "source": [
    "# 您可以更进一步，使用generate为多组消息生成完成结果。这将返回一个带有额外消息参数的LLMResult：\n",
    "\n",
    "batch_meesages = [\n",
    "    [\n",
    "        SystemMessage(content=\"You are a helpful assistant that translates English to Chinese.\"),\n",
    "        HumanMessage(content=\"I love programming.\")\n",
    "    ],\n",
    "    [\n",
    "        SystemMessage(content=\"You are a helpful assistant that translates English to Chinese.\"),\n",
    "        HumanMessage(content=\"I love artificial intelligence.\")\n",
    "    ]\n",
    "]\n",
    "\n",
    "result = chat.generate(batch_meesages)\n",
    "for r in result:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
