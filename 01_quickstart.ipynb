{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 环境配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# 加载.env\n",
    "load_dotenv('.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置openai api key\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Language Model Application: LLMs（构建语言模型应用程序：LLMs）\n",
    "既然我们已经安装了LangChain并设置好了环境，那么我们就可以开始构建我们的语言模型应用程序。\n",
    "\n",
    "LangChain提供了许多模块，可用于构建语言模型应用程序。这些模块可以组合在一起创建更复杂的应用程序，也可以单独使用于简单的应用。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLMs: Get predictions from a language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "RainbowToes Socks.\n"
     ]
    }
   ],
   "source": [
    "text = \"What would be a good company name for a company that makes colorful socks?\"\n",
    "print(llm(text))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Templates: Manage prompts for LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is a good name for a company that makes colorful socks?'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = PromptTemplate(input_variables=['product'],template=\"What is a good name for a company that makes {product}?\")\n",
    "prompt.format(product=\"colorful socks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['product'], output_parser=None, partial_variables={}, template='What is a good name for a company that makes {product}?', template_format='f-string', validate_template=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt.save(\"prompt.json\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chains: Combine LLMs and prompts in multi-step workflows(在多步骤工作流中结合LLMs和提示)\n",
    "\n",
    "到目前为止，我们只是单独使用了PromptTemplate和LLM原语。但实际应用程序不仅仅是一个原语，而是它们的组合。\n",
    "\n",
    "在LangChain中，链由链接组成，可以是像LLMs这样的基本元素或其他链。\n",
    "\n",
    "最核心的链类型是LLMChain，它由PromptTemplate和LLM组成。\n",
    "\n",
    "扩展之前的示例，我们可以构建一个LLMChain来接收用户输入、使用PromptTemplate格式化输入，并将格式化后的响应传递给LLM。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(openai_api_key=OPENAI_API_KEY,temperature=0.9)\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=['product'],\n",
    "    template=\"What is a good name for a company that makes {product}?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = LLMChain(llm=llm,prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nSocksational Colors.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"colorful socks\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agents: Dynamically Call Chains Based on User Input(根据用户输入动态调用链)\n",
    "\n",
    "到目前为止，我们看过的链条都是按照预定顺序运行的。\n",
    "\n",
    "现在代理不再这样做：它们使用LLM来确定采取哪些操作以及以什么顺序进行。一个动作可以是使用工具并观察其输出，或者返回给用户。\n",
    "\n",
    "如果正确使用代理，它们可以非常强大。在本教程中，我们将向您展示如何通过最简单、最高级别的API轻松地使用代理。\n",
    "\n",
    "为了加载代理程序，您应该了解以下概念：\n",
    "\n",
    "- 工具(Tool)：执行特定职责的函数。这可以是像Google搜索、数据库查找、Python REPL和其他链条之类的东西。工具的接口目前是期望有一个字符串作为输入，并且有一个字符串作为输出的函数。\n",
    "\n",
    "- LLM：支持代理程序的语言模型。\n",
    "\n",
    "- 代理程序(Agent)：要使用的代理程序。这应该是引用支持代理类别（class） 的字符串。因为本笔记本专注于最简单、最高级别API 的使用方式，所以只涵盖了标准支持代理程序（standard supported agents） 的使用方法 。如果您想实现自定义  代码，请参阅自定义代码文档 （即将推出） 。\n",
    "\n",
    "Agents: 支持Agent及其规格列表，请参见此处\n",
    "\n",
    "Tools: 预定义工具及其规格列表，请参见此处"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install google-search-results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置serp api key\n",
    "os.environ['SERPAPI_API_KEY'] = os.getenv('SERPAPI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools,initialize_agent,AgentType\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to find the temperature in Fahrenheit and then calculate the 0.23 power of that number.\n",
      "Action: Search\n",
      "Action Input: \"Beijing temperature yesterday in Fahrenheit\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mBeijing Temperature Yesterday. Maximum temperature yesterday: 65 °F (at 2:00 am) Minimum temperature yesterday: 57 °F (at 8:00 pm)\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I need to calculate the 0.23 power of 65\n",
      "Action: Calculator\n",
      "Action Input: 65^0.23\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 2.6119813521152198\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: 2.6119813521152198\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.6119813521152198'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 首先，让我们加载语言模型来控制代理。\n",
    "llm = OpenAI(temperature=0)\n",
    "# 接下来，让我们加载一些需要使用的工具。请注意，“llm-math” 工具使用了 LLM，因此我们需要传递该参数。\n",
    "tools = load_tools([\"serpapi\",\"llm-math\"],llm=llm)\n",
    "# 最后，让我们使用工具、语言模型和所需的代理类型来初始化一个代理。verbose 设置是否显示详细信息\n",
    "agent = initialize_agent(tools=tools,llm=llm,agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,verbose=True)\n",
    "\n",
    "# 昨天旧金山的最高气温是多少华氏度？这个数字的0.23次方是多少？\n",
    "# agent.run(\"What was the high temperature in SF yesterday in Fahrenheit? What is that number raised to the .023 power?\")\n",
    "agent.run(\"昨天北京的最高气温是多少华氏度？这个数字的0.23次方是多少？\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory: Add State to Chains and Agents（为链和代理添加状态）\n",
    "\n",
    "到目前为止，我们所使用的所有链和代理都是无状态的。但通常情况下，您可能希望某个链或代理具有一些“记忆”概念，以便它可以记住有关其先前交互的信息。最清晰和简单的例子就是设计聊天机器人 - 您希望它记住以前的消息，以便可以利用上下文进行更好的对话。这将是一种“短期记忆”。在更复杂的方面，您可以想象一个链/代理随着时间推移记住重要信息 - 这将是一种“长期记忆”的形式。有关后者更具体的想法，请参见这篇精彩论文。\n",
    "\n",
    "LangChain提供了几个专门为此目的创建的链。本笔记本演示如何使用其中之一（ConversationChain）来实现两种不同类型的内存。\n",
    "\n",
    "默认情况下，ConversationChain具有一种简单类型的内存，它会记录所有先前输入/输出并将其添加到传递给它们上下文中。让我们看看如何使用此链（设置verbose=True以便查看提示）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: 你好\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' 你好！很高兴见到你！我是一个智能AI，我可以回答你的问题，或者我们可以聊聊天？'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import OpenAI,ConversationChain\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "conversation = ConversationChain(llm=llm,verbose=True)\n",
    "\n",
    "conversation.predict(input=\"你好\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: 你好\n",
      "AI:  你好！很高兴见到你！我是一个智能AI，我可以回答你的问题，或者我们可以聊聊天？\n",
      "Human: 我的名字叫：ZRRJDD\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' 嗨，ZRRJDD！很高兴认识你！你有什么想聊的？'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"我的名字叫：ZRRJDD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: 你好\n",
      "AI:  你好！很高兴见到你！我是一个智能AI，我可以回答你的问题，或者我们可以聊聊天？\n",
      "Human: 我的名字叫：ZRRJDD\n",
      "AI:  嗨，ZRRJDD！很高兴认识你！你有什么想聊的？\n",
      "Human: 你知道我的名字吗？\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' 不，我不知道你的名字。但是我很乐意认识你！'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"你知道我的名字吗？\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Language Model Application: Chat Models（构建语言模型应用程序：聊天模型）\n",
    "同样，您可以使用聊天模型代替LLMs。 聊天模型是语言模型的一种变体。虽然聊天模型在底层使用语言模型，但它们公开的接口略有不同：它们公开了一个“输入为聊天消息，输出也为聊天消息”的接口，而不是“文本输入、文本输出”的API。\n",
    "\n",
    "由于聊天模型API相对较新，因此我们仍在摸索正确的抽象方法。\n",
    "\n",
    "## Get Message Completions from a Chat Model（从聊天模型中获取消息完成度）\n",
    "您可以通过向聊天模型传递一个或多个消息来获取聊天完成度。响应将是一条消息。目前在LangChain中支持的消息类型有AIMessage、HumanMessage、SystemMessage和ChatMessage——ChatMessage接受任意角色参数。大多数情况下，您只需要处理HumanMessage、AIMessage和SystemMessage。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import AIMessage,HumanMessage,SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='我喜欢编程。', additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 您可以通过传入单个消息来获得完成。\n",
    "chat([HumanMessage(content=\"Translate this sentence from English to Chinese. I love programming.\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='我喜欢编程。', additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 您还可以为OpenAI的gpt-3.5-turbo和gpt-4模型传入多个消息。\n",
    "messages =[\n",
    "    SystemMessage(content=\"You are a helpful assistant that translates English to Chinese.\"),\n",
    "    HumanMessage(content=\"I love programming.\")\n",
    "]\n",
    "chat(messages=messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('generations', [[ChatGeneration(text='我喜欢编程。', generation_info=None, message=AIMessage(content='我喜欢编程。', additional_kwargs={}, example=False))], [ChatGeneration(text='我喜欢人工智能。', generation_info=None, message=AIMessage(content='我喜欢人工智能。', additional_kwargs={}, example=False))]])\n",
      "('llm_output', {'token_usage': {'prompt_tokens': 57, 'completion_tokens': 19, 'total_tokens': 76}, 'model_name': 'gpt-3.5-turbo'})\n"
     ]
    }
   ],
   "source": [
    "# 您可以更进一步，使用generate为多组消息生成完成结果。这将返回一个带有额外消息参数的LLMResult：\n",
    "\n",
    "batch_meesages = [\n",
    "    [\n",
    "        SystemMessage(content=\"You are a helpful assistant that translates English to Chinese.\"),\n",
    "        HumanMessage(content=\"I love programming.\")\n",
    "    ],\n",
    "    [\n",
    "        SystemMessage(content=\"You are a helpful assistant that translates English to Chinese.\"),\n",
    "        HumanMessage(content=\"I love artificial intelligence.\")\n",
    "    ]\n",
    "]\n",
    "\n",
    "result = chat.generate(batch_meesages)\n",
    "for r in result:\n",
    "    print(r)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 聊天提示模版\n",
    "\n",
    "与 LLM 类似，您可以通过使用 `MessagePromptTemplate`来使用模板。 可以从一个或多个 `MessagePromptTemplate` 生成 `ChatPromptTemplate`。 您可以使用 `ChatPromptTemplate` 的 `format_tip` ——这将返回一个 `PromptValue`， 您可以将其转换为字符串或 `Message` 对象，具体取决于您是想将格式化的值用作 `llm` 或聊天模型的输入。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.chat import ChatPromptTemplate,SystemMessagePromptTemplate,HumanMessagePromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='我喜欢编程。', additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = ChatOpenAI(temperature=0)\n",
    "template = \"You are a helpful assistant htat translate {input_language} to {output_language}.\"\n",
    "system_template_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "\n",
    "human_template = \"{text}\"\n",
    "human_templdate_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_template_prompt,human_templdate_prompt])\n",
    "\n",
    "chat(chat_prompt.format_prompt(input_language=\"English\",output_language=\"Chinese\",text=\"I love programming\").to_messages())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m chat_prompt\u001b[39m.\u001b[39;49msave(\u001b[39m\"\u001b[39;49m\u001b[39mprompt_chat.json\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/python39/lib/python3.9/site-packages/langchain/prompts/chat.py:200\u001b[0m, in \u001b[0;36mChatPromptTemplate.save\u001b[0;34m(self, file_path)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msave\u001b[39m(\u001b[39mself\u001b[39m, file_path: Union[Path, \u001b[39mstr\u001b[39m]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 200\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "chat_prompt.save(\"prompt_chat.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# human_templdate_prompt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 带聊天模型的链接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ChatOpenAI' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m chat \u001b[39m=\u001b[39m ChatOpenAI(temperature\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m      2\u001b[0m template \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mYou are a helpful assistant htat translate \u001b[39m\u001b[39m{input_language}\u001b[39;00m\u001b[39m to \u001b[39m\u001b[39m{output_language}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m system_template_prompt \u001b[39m=\u001b[39m SystemMessagePromptTemplate\u001b[39m.\u001b[39mfrom_template(template)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ChatOpenAI' is not defined"
     ]
    }
   ],
   "source": [
    "chat = ChatOpenAI(temperature=0)\n",
    "template = \"You are a helpful assistant htat translate {input_language} to {output_language}.\"\n",
    "system_template_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "\n",
    "human_template = \"{text}\"\n",
    "human_templdate_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_template_prompt,human_templdate_prompt])\n",
    "\n",
    "# chat(chat_prompt.format_prompt(input_language=\"English\",output_language=\"Chinese\",text=\"I love programming\").to_messages())\n",
    "\n",
    "chain = LLMChain(llm=chat,prompt=chat_prompt)\n",
    "\n",
    "chain.run(input_language=\"English\",output_language=\"Chinese\",text=\"I love programming\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 具有聊天模型的代理[](https://www.langchain.com.cn/getting_started/getting_started#%E5%85%B7%E6%9C%89%E8%81%8A%E5%A4%A9%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BB%A3%E7%90%86)\n",
    "\n",
    "代理也可以与聊天模型一起使用，您可以使用 `AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION`作为代理类型来初始化一个聊天模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools,initialize_agent,AgentType\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "'''\n",
    "TODO 请问上面的OpenAI 和ChatOpenAI 有什么区别？\n",
    "chat_models 和 llms 请问这两个包有什么区别？\n",
    "\n",
    "下面为啥 有llm（OpenAI）也有 ChatOpenAI\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I need to use a search engine to find Olivia Wilde's boyfriend and a calculator to raise his age to the 0.23 power.\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Search\",\n",
      "  \"action_input\": \"Olivia Wilde boyfriend\"\n",
      "}\n",
      "```\n",
      "\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mSudeikis and Wilde's relationship ended in November 2020. Wilde was publicly served with court documents regarding child custody while she was presenting Don't Worry Darling at CinemaCon 2022. In January 2021, Wilde began dating singer Harry Styles after meeting during the filming of Don't Worry Darling.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI need to use a search engine to find Harry Styles' current age.\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Search\",\n",
      "  \"action_input\": \"Harry Styles age\"\n",
      "}\n",
      "```\n",
      "\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m29 years\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mNow I need to calculate 29 raised to the 0.23 power.\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Calculator\",\n",
      "  \"action_input\": \"29^0.23\"\n",
      "}\n",
      "```\n",
      "\n",
      "\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 2.169459462491557\u001b[0m\n",
      "Thought:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-o6vZJ1hLHX7XoaQvFodPY9Ty on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 2.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-o6vZJ1hLHX7XoaQvFodPY9Ty on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-o6vZJ1hLHX7XoaQvFodPY9Ty on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-o6vZJ1hLHX7XoaQvFodPY9Ty on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mI now know the final answer.\n",
      "Final Answer: 2.169459462491557\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.169459462491557'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = ChatOpenAI(temperature=0)\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "tools = load_tools([\"serpapi\",\"llm-math\"],llm=llm)\n",
    "\n",
    "agent = initialize_agent(tools=tools,llm=chat,agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION,verbose=True)\n",
    "\n",
    "# 奥利维亚·王尔德的男友是谁？他的年龄现在提高到0.23次方是多少？\n",
    "agent.run(\"Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 内存: 向链和代理添加状态[](https://www.langchain.com.cn/getting_started/getting_started#%E5%86%85%E5%AD%98-%E5%90%91%E9%93%BE%E5%92%8C%E4%BB%A3%E7%90%86%E6%B7%BB%E5%8A%A0%E7%8A%B6%E6%80%81-1)\n",
    "\n",
    "您可以对链使用 Memory，对代理使用聊天模型进行初始化。这与 LLM 的 Memory 之间的主要区别在于，我们不需要将以前的所有消息压缩成一个字符串，而是可以将它们保留为自己独特的内存对象。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate,SystemMessagePromptTemplate,HumanMessagePromptTemplate,MessagesPlaceholder\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'你好！我是一名AI，很高兴能和你交流。有什么我可以帮助你的吗？'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    # SystemMessagePromptTemplate.from_template(\"The following is a friendly conversation between a human and AI. The AI is talk\"),\n",
    "    SystemMessagePromptTemplate.from_template(\"以下是人类和AI之间友好的对话。 AI健谈并提供了许多来自其上下文的具体细节。如果AI不知道问题的答案，它会诚实地说出自己不知道。\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"{input}\")\n",
    "])\n",
    "\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "\n",
    "conversation = ConversationChain(memory=memory,prompt=prompt,llm=llm)\n",
    "# conversation.predict(input=\"hi there!\")\n",
    "conversation.predict(input=\"你好\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'没问题，我很高兴能和你聊天。有什么话题你想聊吗？'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# conversation.predict(input=\"I'm doing well! Just having a conversation with an AI.\")\n",
    "conversation.predict(input=\"我很好！只是在与人工智能聊天。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'当然，我是一名人工智能，我的名字是OpenAI。我被设计成能够理解和回答各种问题，从简单的数学问题到更复杂的哲学问题。我使用自然语言处理技术来理解你的问题，并尝试给出最准确的答案。我还可以学习和改进自己的回答，以便更好地为你服务。'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# conversation.predict(input=\"Tell me about yourself.\")\n",
    "conversation.predict(input=\"介绍一下你自己.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
